{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure sigmoid_saturation_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f88f2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha = 0.01):\n",
    "    return np.maximum(alpha * z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12850e7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9022\n",
      "5 Batch accuracy: 0.9 Validation accuracy: 0.9436\n",
      "10 Batch accuracy: 0.96 Validation accuracy: 0.9598\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.9662\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9698\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9738\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9754\n",
      "35 Batch accuracy: 0.96 Validation accuracy: 0.9758\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha = 1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure elu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3Xl8FeW9x/HPLwnKKiBorCJgXVDrwhWq1bqkalUWt7q2asUNKtqWqq0b9Gql2ipWqApKixcFF1CwKgh41XvABaWgIFAFRECQfTlAgARInvvHc4CQ9SSZZOac832/XueVyTxzZn5nGM43sz1jzjlERESiJivsAkRERMqjgBIRkUhSQImISCQpoEREJJIUUCIiEkkKKBERiSQFlIiIRJICSkREIkkBJSnDzIab2bg0Wk6WmT1rZuvMzJlZXl0vs5Ja6uUzJ5bV0sxWmdnh9bG86jKzV83szrDrEDD1JJGezGw4cH05TZ86536UaG/tnOtewftjwBzn3O2lxvcAnnLONQ204OSW3Ry/zcZTaTmVLL87MBbIA74B1jvnttflMhPLjVHqc9fXZ04s6zH8tndDXS+rnGWfCdwFdAIOBm5wzg0vNc3xwGTgMOfcxvquUfbICbsAqVPvAteVGlfnX4B1pb6+LOrxS+kIYIVz7uN6Wl6F6uszm1lj4GbgwvpYXjmaAnOAFxKvMpxzs83sG+Ba4Ol6rE1K0SG+9FbonFtZ6rW+rhdqZheY2QdmtsHM1pvZJDM7pkS7mdmdZrbAzArNbJmZPZJoGw6cBdyWOOzlzKz9rjYzG2dmPROHiLJLLfclM3szmTqSWU6J+exrZgMTyywws0/M7PQS7TEzG2xmD5vZWjNbbWYDzKzC/1+J5T8BtE0se3GJeT1Vetpd9SSzrJqs3+p+5pp+bqAr4ICPylknnczsPTPbZmZfm9mZZnalmZWZtqacc2875+5zzr0GFFcy6ZvAz4NartSMAkrqQhNgIHAy/vDVRuAtM9sn0f4w0A94BPgBcAWwNNH2W2Aq8D/A9xKvXW27vAo0B366a4SZNQUuBkYmWUcyy9nlUeAq4Ebgv4DZwEQz+16Jaa4BdgKnAbcDfRLvqchvgT8ByxLL/mEl05ZW1bJqu34huc+cTC2lnQHMcKXOLZjZD4EPgP8DTgA+AR4E7k98FkpNf5+Z5VfxOqOSOqoyDTjZzBrVYh5SSzrEl94uMLP8UuOeds7dXZcLdc6NKfm7md0AbML/h58J/A7o45x7LjHJ1/gvTZxzG81sO7DVObeygvlvMLO38V+OExOjL8F/Ub5ZYroK63DOfVjVchLvaQLcCtzsnBufGPcr4GzgNqBvYtL/OOf+mBieb2a3AOcAL1fwGTaa2WagqLLlV6DCZSWCutrr18xq8pmr/bmBdsDycsY/DrzlnOufWN5LwFvAFOfc++VM/wwwuoJl7PJdFe2VWQ40wJ+nWliL+UgtKKDS2xSgZ6lx9XES/HDgIeAU4AD8nnoW0BZ/Dmxf4L1aLmYk8LyZNXbObcWH1RjnXEGSdSTrcPwX1e7DTM65IjObChxbYrovSr1vOXBgNZZTHZUt61hqv36T/cxV1VKeRsCqkiPM7CD8ntVPSozejv+3KrP3lKhnPVCXh6u3JX5qDypECqj0ttU593UN37sJfxittBb4Q2WVGYc/dNUL/1fsTuA/wD6Vvamaxifme7GZvQecC5xfz3WUPEy1o5y2mhxCLwas1LgGpX4Palk1Ufqy3+rWshZoWWrcrvOT00uM6wDMc859WN5MzOw+4L7KS6WLc+6DKqapyP6Jn2tq+H4JgAJKKjIP6GpmVup8wUmJtnKZWSvgaKC3c+7/EuNOYs+29iVQiD8MtKCC2WwHsitoA8A5V2hmr+L3nFoDK4FYNepIajn4wzvbgR8nhjF/ccapwEtVvLcm1uDPC5V0IrA4yfcHsX7r8jN/DvQoNa4FPtiKEstqhj/3VNmhz7o+xHcc8J1zblWVU0qdUUClt30Th09KKnLO7fqrcD8z61iqPe6cWwwMwZ/0ftLM/gEU4K/A+jlwUSXL3ID/K/kWM1sKHAI8ht97wTm32cwGAY+YWSH+MGQroJNzbkhiHovx56vaA/n4+4PKu+JqJP5Q1mHAy6WmqbSOZJfjnNtiZkOAv5rZWmAR/hxPLjC4kvVQU+8DA83sIvwfAr2AQ0kyoGq6fkvNoy4/86TEfFs559Ylxs3E7zXea2Yv4v+dVgBHmNmRzrkyQVvTQ3yJc3RHJH7Nwl9F2RH/b/9tiUnPSNQqIdJVfOntXPx/9JKvz0u0n5H4veRrAIBz7hvgTOBI4B38VU1XA1c45yZUtMDEF/xV+Cux5uDvI+mH/6t+l3uBvybGfwmMAdqUaB+A/wv+P/g9iorOGX2A/yv5WPa+ei/ZOpJdzt3AKPyVbzMT87zAObeigulr47kSr4+AzcDr1ZxHEOu3Tj6zc242e7alXeMW4feYbgVm4T/zufh/t6DvEevMnm29Ef5Kwc/xV1QCYGYNgUuBfwS8bKkm9SQhIvXKzC4ABgHHOueKwq6nNDO7DbjYOXde2LVkOu1BiUi9cs5NxO/Rtqlq2pDsAH4ddhGiPSgREYko7UGJiEgkKaBERCSSQr/MvHXr1q59+/Zhl1HGli1baNKkSdhlpBSts+TNmzePoqIijj22dMcMUpFU276WLIG1ayE7Gzp0gEYh9EkR1XU2Y8aMtc65A6qaLvSAat++PdOnT696wnoWi8XIy8sLu4yUonWWvLy8POLxeCS3/ahKpe3rj3+Ehx6Chg3h3Xfhxz8Op46orjMzW5LMdDrEJyISoKef9uGUnQ2jR4cXTulAASUiEpBXX4VfJy5QHzoULgzrsYxpQgElIhKA99+Ha68F5+Dhh+HGG8OuKPUFGlBmNtLMVpjZJjObb2Y3Bzl/EZEo+vxzuOQS2L4dfvMbuOeesCtKD0HvQT0CtHfO7YfvULS/mXUKeBkiIpGxcCF06QKbN8NVV8ETT4CVfmCK1EigAeWcm+uc29UZp0u8Dg9yGSIiUbFqFZx/vv957rnw/POQpRMngQn8MnMzG4x/3ksjfC/Bb5czTU8ST3rNzc0lFosFXUat5efnR7KuKNM6S148HqeoqEjrqxqitn1t3ZpNnz4dWbiwGUceuZnf/W4mU6dGq+/bqK2z6qqTvvhKPNwsD/irc670Uzd369y5s4vivSBRvX8gyrTOkrfrPqiZM2eGXUrKiNL2VVgI3brBe+/B4YfDRx9Bbm7YVZUVpXVWkpnNcM51rmq6OtkZdc4VJR7V3Ab/jBcRkbRQXAzXX+/DKTcXJk2KZjilg7o+WpqDzkGJSJpwDvr0gVGjoFkzmDDB70FJ3QgsoMzsQDO72syamlm2mZ2Pfzz4e0EtQ0QkTH/5Czz5JOyzD/zrX/Bf/xV2RektyIskHP5w3jP44FsC9HHOvRngMkREQjFsGNx3n7+EfORIOPvssCtKf4EFlHNuDXBWUPMTEYmKN9+Enj398FNPwRVXhFtPptAV+yIilfjoI38DbnEx9OsHvXuHXVHmUECJiFRg7lzo3h0KCuCWW+DBB8OuKLMooEREyrF0KVxwAcTjcPHFMHiwujCqbwooEZFS1q3zXRgtWwannw4vvww5oT/eNfMooEREStiyxR/W+/JLOO44f4FEGI9rFwWUiMhuO3b4CyI++QTatoWJE6Fly7CrylwKKBERfC8Rt9wC48dDq1a+C6NDDgm7qsymgBIRAe691z8uo3FjH1JHHx12RaKAEpGM98QT8Ne/+gshxoyBU04JuyIBBZSIZLgXX4Q77vDDzz3nLy2XaFBAiUjGeucd6NHDDw8YANddF2o5UooCSkQy0r//DT/7GezcCXfe6V8SLQooEck48+dD167+nqdrr4VHHw27IimPAkpEMsqKFb6XiLVr/fmm556DLH0TRpL+WUQkY2zc6ENp8WI4+WR49VVo0CDsqqQiCigRyQgFBb7T1y++gA4d/L1OTZuGXZVURgElImmvqAiuuQYmT4aDD/a9RLRuHXZVUhUFlIikNefgtttg7Fho3tz3r9euXdhVSTIUUCKS1v70J3j2Wdh3X3jrLTj++LArkmQpoEQkbT37LDzwgL9K75VX4Iwzwq5IqkMBJSJpaexY6N3bDw8ZApdcEm49Un0KKBFJO5Mnwy9+AcXF/hBfz55hVyQ1oYASkbQyaxZcdBEUFvo9qL59w65IakoBJSJpY9EifyPupk1w+eXw97+DWdhVSU0poEQkLaxZ47swWrkSfvITGDkSsrPDrkpqQwElIikvP993/rpgAXTsCK+/7i8rl9SmgBKRlLZ9O1x2GUyfDocdBhMm+BtyJfUpoEQkZRUXww03+AcPHnCA/3nQQWFXJUFRQIlISnIO7roLXnrJd/o6YQIccUTYVUmQFFAikpIGDIAnnvCPyxg7Fjp1CrsiCZoCSkRSzvPPwx/+4IdfeAF++tNw65G6oYASkZQyfjzcdJMfHjQIrr463Hqk7gQWUGa2r5kNM7MlZrbZzGaaWZeg5i8i8skncMUV/vlO994Lv/lN2BVJXQpyDyoHWAqcBTQH+gKjzax9gMsQkQy1ZEljunWDbdvgxhvhz38OuyKpazlBzcg5twV4oMSocWa2COgELA5qOSKSeZYtgz/84QTWr4fu3f1jNNSFUfqrs3NQZpYLHAXMratliEj627DB96+3enVDTjsNRo2CnMD+tJYoq5N/ZjNrALwIPO+c+6qc9p5AT4Dc3FxisVhdlFEr+fn5kawryrTOkhePxykqKtL6qkJhYRZ33XUic+c259BDN3P33bOYNm1n2GWljFT/Pxl4QJlZFjAC2A7cXt40zrmhwFCAzp07u7y8vKDLqLVYLEYU64oyrbPktWjRgng8rvVViZ07fRdGc+ZAmzYwYMAcLrro9LDLSimp/n8y0IAyMwOGAblAV+fcjiDnLyKZwTn41a/gzTehZUuYNAlWry4MuyypZ0GfgxoCHANc6JzbFvC8RSRD9OsHw4ZBo0b+vqdjjw27IglDkPdBtQN6AR2BlWaWn3hdE9QyRCT9Pfmkv4Q8OxtGj4ZTTw27IglLkJeZLwF04aeI1NioUfDb3/rhf/7TX1IumUtdHYlIJLz7Llx3nT//9Je/QI8eYVckYVNAiUjoPvsMLr0UduyAPn32dAQrmU0BJSKhWrgQunTxj23/+c/h8cfVS4R4CigRCc2qVXDeebB6tX9kxvDhkKVvJUnQpiAiodi0ye85ffONf9jgmDGwzz5hVyVRooASkXpXWOjPOX3+uX9M+9tvQ7NmYVclUaOAEpF6VVTkr9Z7/3046CB45x048MCwq5IoUkCJSL1xzt/n9OqrsN9+MGECHHZY2FVJVCmgRKTePPwwPP20P9f0xhvQsWPYFUmUKaBEpF7885/Qt6+/hPyllyCFO9mWeqKAEpE698Yb0KuXHx482D9GQ6QqCigRqVMffghXXw3FxfDf/+0foyGSDAWUiNSZOXPgwguhoAB69vQBJZIsBZSI1IklS+D88yEe9/c8DR6sLoykehRQIhK4tWt9OC1fDmee6S+KyM4OuypJNQooEQnUli3+OU7z5sHxx/sLJBo2DLsqSUUKKBEJzI4dcMUV8Omn0K4dTJwILVqEXZWkKgWUiASiuBhuusn3DtG6te/C6OCDw65KUpkCSkQCcc89MGIENGkC48fDUUeFXZGkOgWUiNTa3/4Gjz0GOTn+sRknnxx2RZIOFFAiUisvvgh33umHhw/3V++JBEEBJSI1NmkS9Ojhhx9/HK65JtRyJM0ooESkRqZN833q7dwJv/893HFH2BVJulFAiUi1zZsH3br5e55++Uv4y1/CrkjSkQJKRKpl+XJ/nmntWujSxT9GI0vfJFIHtFmJSNLicbjgAt/P3imn+CfjNmgQdlWSrhRQIpKUbdvgootg9mw4+mh/r1OTJmFXJelMASUiVSoq8lfoffABHHKIv3qvVauwq5J0p4ASkUo5B717w+uv+371Jk6Etm3DrkoygQJKRCr14IMwdKjvkfytt+C448KuSDKFAkpEKjRkiA+orCwYNQpOPz3siiSTKKBEpFyvvQa33eaHn33WXyAhUp8UUCJSRizmL4pwDvr3h5tvDrsiyUSBBpSZ3W5m082s0MyGBzlvEakfM2fCxRfD9u1w++1w331hVySZKifg+S0H+gPnA40CnreI1LFvvvG9Q2zaBFdeCQMHglnYVUmmCjSgnHNjAcysM9AmyHmLSN1avdp3YbRyJZx9NrzwAmRnh12VZLKg96CSYmY9gZ4Aubm5xGKxMMqoVH5+fiTrijKts+TF43GKioois762bs3mjjtO5Ouv9+PIIzdzxx0zmTq1KOyy9qLtq/pSfZ2FElDOuaHAUIDOnTu7vLy8MMqoVCwWI4p1RZnWWfJatGhBPB6PxPravh26d/c9lB9+OHzwQTNyc88Iu6wytH1VX6qvM13FJ5LBiov9Awf/93/hwAN9F0a5uWFXJeIpoEQylHP+IYMvvwxNm8KECX4PSiQqAj3EZ2Y5iXlmA9lm1hDY6ZzbGeRyRKT2Hn0UBg3yj8v417/gpJPCrkhkb0HvQfUFtgH3ANcmhvsGvAwRqaX/+R+45x5/CfnIkXDOOWFXJFJW0JeZPwA8EOQ8RSRY48bBLbf44UGD/P1OIlGkc1AiGWTqVB9IRUVw//3w61+HXZFIxRRQIhniP/+Bbt38k3FvugkeeijsikQqp4ASyQBLl/peIjZs8L2SP/OMujCS6FNAiaS59evhggtg2TL/PKdXXoGcUG7RF6keBZRIGtu6FS680B/e+8EP4M03oZG6cZYUoYASSVM7d8JVV8HHH8Ohh8LEidCyZdhViSRPASWShpyDnj39JeX77++7MGqj5wtIilFAiaSh++/3N+M2agTjx8Mxx4RdkUj1KaBE0sygQfDII/5ZTq+9Bj/6UdgVidSMAkokjbzyCvTp44efew66dg23HpHaUECJpIl334Vf/tIPP/ronmGRVKWAEkkDM2bApZfCjh3+ERp33RV2RSK1p4ASSXELFkCXLpCfD9dcA489pl4iJD0ooERS2MqVvgujNWvgvPP8eacs/a+WNKFNWSRFbdzouzBatAh++EMYMwb22SfsqkSCo4ASSUEFBXDJJTBrFhx5pL/XqWnTsKsSCZYCSiTFFBXBdddBLAbf+x688w4ccEDYVYkETwElkkKcg9/8xt+Au99+vn+99u3DrkqkbiigRFLIn/8MgwfDvvv6nslPOCHsikTqjgJKJEX84x/Qr5+/Su+ll+Css8KuSKRuKaBEUsC//gW/+pUfHjwYfvazcOsRqQ8KKJGImzIFrr4aiovhwQehV6+wKxKpHwookQibPRsuuggKC/0eVL9+YVckUn8UUCIRtXix7yVi40Z/SO+pp9SFkWQWBZRIBK1d68NpxQp/McSLL/rnO4lkEgWUSMTk50O3bjB/Ppx4IrzxBjRsGHZVIvVPASUSITt2wOWXw7Rp/gbcCROgefOwqxIJhwJKJCKKi+HGG2HSJN910Tvv+K6MRDKVAkokIu6+G0aOhCZN4O23fSewIplMASUSAQMG+FeDBvD669C5c9gViYRPASUSshEj4Pe/98PPPw8//Wm49YhEhQJKJEQTJvjzTgBPPAE//3m49YhESaABZWb7m9nrZrbFzJaY2S+CnL9IOtm6NZvLL4edO/35pz59wq5IJFpyAp7f08B2IBfoCIw3s1nOubkBL0ckpW3dCt9805SiIrj+enjkkbArEokec84FMyOzJsAG4Djn3PzEuBHAd865eyp6X7NmzVynTp0CqSFI8XicFi1ahF1GStE6S05BAUybNhPnYP/9O3LccerCKBnavqovquts8uTJM5xzVV4KFOQe1FHAzl3hlDALKPPUGjPrCfQEaNCgAfF4PMAyglFUVBTJuqJM66xqO3dmsWBBU5yDrCzHIYdsZOPGYP5ITHfavqov1ddZkAHVFNhUatxGoFnpCZ1zQ4GhAJ07d3bTp08PsIxgxGIx8vLywi4jpWidVS4e9/3qbd8OTZvm0b79Rr744vOwy0oZ2r6qL6rrzJI8ZBBkQOUD+5Uatx+wOcBliKSkjRuha1f44gvo0AFatYItW7TnJFKZIK/imw/kmFnJ+99PBHSBhGS0DRv8vU1Tp0Lbtr4LowYNwq5KJPoCCyjn3BZgLPAnM2tiZj8GLgZGBLUMkVSzdi2ccw78+99w2GEwebIPKRGpWtA36vYGGgGrgZeBW3WJuWSq1avh7LPh8899v3qTJ/seykUkOYHeB+WcWw9cEuQ8RVLRwoXQpQssWABHHw3vv6+eyUWqS10diQRs2jQ49VQfTh07QiymcBKpCQWUSIDeegvy8mDNGjjvPJgyBXJzw65KJDUpoEQC4Bz8/e9wySWwbRvccAOMGwfNytwFKCLJUkCJ1NK2bdCjB/z2t/6puH/8IwwbpkvJRWor6M5iRTLKkiXws5/BZ59B48Y+mK6+OuyqRNKDAkqkhiZNgmuv9fc6ff/7/km4J5wQdlUi6UOH+ESqqbAQ7rwTLrjAh9P55/sbcRVOIsHSHpRINcyb5596+/nnkJ0NDz0Ef/iDHxaRYCmgRJJQVARPPQX33ecfNvj978NLL8Epp4RdmUj6UkCJVOHLL+Gmm3xnrwDXXefDar/SffeLSKB0DkqkAoWF0L+/7w1i6lQ4+GB44w144QWFk0h90B6USDnGj4c+feDrr/3vN98Mjz0GEXx6tkjaUkCJlLBgAfzudz6gAI45xh/OO/vscOsSyUQ6xCcCrFwJt90Gxx7rw6lZM/jb32DWLIWTSFi0ByUZLR6HAQPgiSf81XlZWb4fvYcfhoMOCrs6kcymgJKMtHYtDBoETz4JGzf6cRdfDH/+M/zgB+HWJiKeAkoyynff+b2lIUP8HhPAT37ig+nUU8OtTUT2poCSjDBtGgwcCK++Cjt3+nFdu8L998Npp4Vbm4iUTwElaWvLFnjtNXjmGfjkEz8uOxuuvBLuvhtOOinc+kSkcgooSSvOwaef+sdejBoFmzf78S1bQs+e/kq9Qw8Nt0YRSY4CStLCt9/C6NHw3HO+a6JdTjsNbrzRP6OpSZPw6hOR6lNAScpavNgfwnv1VX+OaZcDD4Trr/fBdPTRoZUnIrWkgJKU4Zy/cXbCBBg7FqZP39PWuDF06wa/+IX/qceti6Q+BZRE2oYN8O67PpQmToQVK/a0NWkC3bvDFVdAly4+pEQkfSigJFI2boQPP4TJk2HKFL+XVFS0p/3gg/2TbLt18z8VSiLpSwEloXEOlizx54+mTvWhNGsWFBfvmSYnB846y+8hdekCxx8PZuHVLCL1RwEl9cI534vDF1/4vaJp0/xrzZq9p2vQAH70Ix9KZ54JP/6x77hVRDKPAkoCt2kTzJkDs2fv/dqwoey0rVvDD38IJ58MZ5zhuxvSYTsRAQWU1FBhIXzzjX9+0q7XtGknsnYtLF1a/nv2398fouvUyQfSySdD+/Y6ZCci5VNASbk2bfI3vy5dWvbn4sV+uOS5Iq8lAPvs45+rdPzxe14nnADf+57CSESSp4DKIEVFsG4drFq192v1av9z5UpYtsyHz6ZNlc8rKwu+/3048sg9r23bvuCyy06gXTvdhyQitaeASkGFhb6PuQ0b/Gv9+sp/btjgL0ZYs6a8vZ7yNWoEbdv6fuvK+3nYYX5PqaRYbD1HHBH85xWRzBRIQJnZ7UAP4HjgZedcjyDmm4qcgx07YNs2/yooqHp461bIz/ehs3nznuGKxu3YUfP6WraE3Nw9r4MO2vv3Qw7xAbT//jocJyLhCmoPajnQHzgfaFSdNxYWwvz5/vBT6Vdxcfnjq2qrqn3Hjj2v7dv3/rlr+LvvjmXQoKqn2zW8K3AKCpLfS6mpnBx/6XXLlj5IWrbce7i8ca1a+T7qSu/1iIhEVSAB5ZwbC2BmnYE21XnvnDnz6NAhr9TYK4HewFagaznv6pF4rQUuL6f9VuAqYClwXTntdwIXAvOAXuW09wXOBWYCfcppfxg4DfgYuK9Ma3b2QBo37khW1rsUFPQnK4vdr+xsOP74ZznggA6sW/cW8+Y9TnY2e71uvXUE7dodyowZo5g4cUiZ9rFjX6N169YMHz6c4cOHs337nvNJAG+//TaNGzdm8ODB/P3vo8vUF4vFABgwYADjxo3bq61Ro0ZMmDABgIceeoj33ntvr/ZWrVoxZswYAO69916mTp26uy0ej3PccccxcuRIAPr06cPMmTP3ev9RRx3F0KFDAejZsyfz58/fq71jx44MHDgQgGuvvZZly5bt1X7qqafyyCOPAHDZZZexbt26vdrPOecc+vXrB0CXLl3Ytm3bXu3du3fnrrvuAiAvL6/Murnyyivp3bs3W7dupWvXsttejx496NGjB2vXruXyy8tue7feeitXXXUVS5cu5brrym57d955JxdeeCFbt27l66+/LlND3759Offcc5k5cyZ9+pTd9h5++GFOO+00Pv74Y+67r+y2N3DgQDp27Mi7775L//79y7Q/++yzdOjQgbfeeovHH3+8TPuIESM49NBDGTVqFEOGDCnT/tpre297pZXc9kaPDnbbKy4uZsqUKUDZbQ+gTZs22vZKbXvxeJwWLVoAe7a9efPm0atX2e+9+tz2khXKOSgz6wn09L81YZ99ihOHkxxm0KxZAS1bbga2sGzZzsR79hxyOuCAfA48cB1FReuYP3/H7vFmDoC2beMcfPAKCgtXMWvWdsxciWngmGPW0K7dYrZsWcbUqQW723fVcPrpSzj00M/YvPkbJk3akmhzu39eeumXdOjQgEWL5jJmzKbd47Oy/M9f/3o6RxwRZ8aMWYwYES/z+W+++VPatl3Bxx/PJh4v296mzVRatVpITs5ciovjFBfvfVjvo48+onnz5nz11Vflvn/KlCk0bNiQ+fPnl9u+60ti4cKFZdq3bdu2u33RokVl2ouLi3e3f/vtt3u1FxUVsWrVqt3ty5YtK/P+5cuX725fvnx5mfZly5btbl+1alWZ9m+//XZ3+5o1a9hU6mqORYsW7W5fv349hYWFe7UvXLhwd3t562b+/PlLNR+DAAAF6UlEQVTEYjEKCgrKbf/qq6+IxWJs3Lix3Pa5c+cSi8VYvXp1ue2zZ8+mWbNmbN68GedcmWlmzZpFTk4OX3/9dbnv/+yzz9i+fTtz5swpt3369OnE43FmzZpVbvunn37KihUrmD27/G1v6tSpLFy4kLlz55bbHua217hx4wq3PYAGDRpo2yu17RUVFe0e3rXtlbfuoH63vWSZcy7piaucmVl/oE11zkF17tzZTS/ZLXVExGKxcv/KkYppnSUvLy+PeDxe5q98qZi2r+qL6jozsxnOuc5VTZeVxIxiZuYqeH0YTLkiIiJ7q/IQn3Murx7qEBER2UtQl5nnJOaVDWSbWUNgp3NuZxDzFxGRzFPlIb4k9QW2AfcA1yaG+wY0bxERyUBBXWb+APBAEPMSERGB4PagREREAqWAEhGRSFJAiYhIJCmgREQkkhRQIiISSQooERGJJAWUiIhEkgJKREQiSQElIiKRpIASEZFIUkCJiEgkKaBERCSSFFAiIhJJCigREYkkBZSIiESSAkpERCJJASUiIpGkgBIRkUhSQImISCQpoEREJJIUUCIiEkkKKBERiSQFlIiIRJICSkREIkkBJSIikaSAEhGRSFJAiYhIJCmgREQkkhRQIiISSQooERGJJAWUiIhEkgJKREQiqdYBZWb7mtkwM1tiZpvNbKaZdQmiOBERyVxB7EHlAEuBs4DmQF9gtJm1D2DeIiKSoXJqOwPn3BbggRKjxpnZIqATsLi28xcRkcxU64AqzcxygaOAuZVM0xPoCZCbm0ssFgu6jFrLz8+PZF1RpnWWvHg8TlFRkdZXNWj7qr5UX2fmnAtuZmYNgAnAQudcr2Te07lzZzd9+vTAaghKLBYjLy8v7DJSitZZ8vLy8ojH48ycOTPsUlKGtq/qi+o6M7MZzrnOVU1X5TkoM4uZmavg9WGJ6bKAEcB24PZaVS8iIhmvykN8zrm8qqYxMwOGAblAV+fcjtqXJiIimSyoc1BDgGOAc51z2wKap4iIZLAg7oNqB/QCOgIrzSw/8bqm1tWJiEjGCuIy8yWABVCLiIjIburqSEREIkkBJSIikRTofVA1KsBsDbAk1CLK1xpYG3YRKUbrrHq0vqpH66v6orrO2jnnDqhqotADKqrMbHoyN5LJHlpn1aP1VT1aX9WX6utMh/hERCSSFFAiIhJJCqiKDQ27gBSkdVY9Wl/Vo/VVfSm9znQOSkREIkl7UCIiEkkKKBERiSQFlIiIRJICKklmdqSZFZjZyLBriSoz29fMhpnZEjPbbGYzzaxL2HVFjZntb2avm9mWxLr6Rdg1RZW2qdpJ9e8tBVTyngb+HXYREZcDLAXOApoDfYHRZtY+xJqi6Gn8gz1zgWuAIWb2g3BLiixtU7WT0t9bCqgkmNnVQBx4L+xaosw5t8U594BzbrFzrtg5Nw5YBHQKu7aoMLMmwGVAP+dcvnPuQ+BN4LpwK4smbVM1lw7fWwqoKpjZfsCfgDvCriXVmFkucBQwN+xaIuQoYKdzbn6JcbMA7UElQdtUctLle0sBVbWHgGHOuWVhF5JKzKwB8CLwvHPuq7DriZCmwKZS4zYCzUKoJaVom6qWtPjeyuiAMrOYmbkKXh+aWUfgXOCJsGuNgqrWV4npsoAR+PMst4dWcDTlA/uVGrcfsDmEWlKGtqnkpdP3Vq2fqJvKnHN5lbWbWR+gPfCtmYH/6zfbzI51zp1U5wVGTFXrC8D8ihqGvwCgq3NuR13XlWLmAzlmdqRzbkFi3InokFWFtE1VWx5p8r2lro4qYWaN2fuv3bvw//C3OufWhFJUxJnZM0BH4FznXH7Y9USRmb0COOBm/Lp6GzjNOaeQKoe2qepJp++tjN6DqopzbiuwddfvZpYPFKTaP3J9MbN2QC+gEFiZ+OsNoJdz7sXQCoue3sBzwGpgHf6LQ+FUDm1T1ZdO31vagxIRkUjK6IskREQkuhRQIiISSQooERGJJAWUiIhEkgJKREQiSQElIiKRpIASEZFIUkCJiEgk/T/XSE/diHEg1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x132ff2a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"elu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.elu, name = 'hidden1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure selu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13ea022b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -0.26 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 10: -0.24 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 20: -0.17 < mean < 0.18, 0.74 < std deviation < 1.24\n",
      "Layer 30: -0.27 < mean < 0.24, 0.78 < std deviation < 1.20\n",
      "Layer 40: -0.38 < mean < 0.39, 0.74 < std deviation < 1.25\n",
      "Layer 50: -0.27 < mean < 0.31, 0.73 < std deviation < 1.27\n",
      "Layer 60: -0.26 < mean < 0.43, 0.74 < std deviation < 1.35\n",
      "Layer 70: -0.19 < mean < 0.21, 0.75 < std deviation < 1.21\n",
      "Layer 80: -0.18 < mean < 0.16, 0.72 < std deviation < 1.19\n",
      "Layer 90: -0.19 < mean < 0.16, 0.75 < std deviation < 1.20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "Z = np.random.normal(size = (500, 100))\n",
    "\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size = (100, 100), scale = np.sqrt(1 / 100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    \n",
    "    means = np.mean(Z, axis = 1)\n",
    "    stds = np.std(Z, axis = 1)\n",
    "    \n",
    "    if layer % 10 == 0:\n",
    "        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.selu, name = 'hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.selu, name = 'hidden2')\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name = 'outputs')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = mnist.train.images.mean(axis = 0, keepdims = True)\n",
    "stds = mnist.train.images.std(axis = 0, keepdims = True) + 1e-10\n",
    "\n",
    "means_val = mnist.test.images.mean(axis = 0, keepdims = True)\n",
    "stds_val = mnist.test.images.std(axis = 0, keepdims = True) + 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.98 Validation accuracy: 0.9218\n",
      "5 Batch accuracy: 0.96 Validation accuracy: 0.9528\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9628\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.9646\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.967\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9678\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9684\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict = {X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict = {X: X_batch_scaled, y: y_batch})\n",
    "            X_val_scaled = (mnist.validation.images - means_val) / stds_val\n",
    "            acc_test = accuracy.eval(feed_dict = {X: X_val_scaled, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape = (), name = 'training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name = 'hidden1')\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training = training, momentum = 0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name = 'hidden2')\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training = training, momentum = 0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits = tf.layers.dense(bn2_act, n_outputs, name = 'outputs')\n",
    "\n",
    "# logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name = 'outputs')\n",
    "# logits = tf.layers.batch_normalization(logits_before_bn, training = training, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "training = tf.placeholder_with_default(False, shape = (), name = 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training = training, momentum = 0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name = 'hidden1')\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name = 'hidden2')\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name = 'outputs')\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    # logits = tf.layers.dense(bn2, n_outputs, name = 'outputs')\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "# saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8742\n",
      "1 Test accuracy: 0.8976\n",
      "2 Test accuracy: 0.912\n",
      "3 Test accuracy: 0.9208\n",
      "4 Test accuracy: 0.9276\n",
      "5 Test accuracy: 0.9358\n",
      "6 Test accuracy: 0.9394\n",
      "7 Test accuracy: 0.9435\n",
      "8 Test accuracy: 0.9474\n",
      "9 Test accuracy: 0.9502\n",
      "10 Test accuracy: 0.9536\n",
      "11 Test accuracy: 0.9549\n",
      "12 Test accuracy: 0.9568\n",
      "13 Test accuracy: 0.9584\n",
      "14 Test accuracy: 0.9586\n",
      "15 Test accuracy: 0.9609\n",
      "16 Test accuracy: 0.9615\n",
      "17 Test accuracy: 0.9626\n",
      "18 Test accuracy: 0.9646\n",
      "19 Test accuracy: 0.9653\n",
      "20 Test accuracy: 0.9654\n",
      "21 Test accuracy: 0.9647\n",
      "22 Test accuracy: 0.966\n",
      "23 Test accuracy: 0.9671\n",
      "24 Test accuracy: 0.9675\n",
      "25 Test accuracy: 0.9675\n",
      "26 Test accuracy: 0.9685\n",
      "27 Test accuracy: 0.9677\n",
      "28 Test accuracy: 0.9694\n",
      "29 Test accuracy: 0.9694\n",
      "30 Test accuracy: 0.9697\n",
      "31 Test accuracy: 0.9699\n",
      "32 Test accuracy: 0.9695\n",
      "33 Test accuracy: 0.9699\n",
      "34 Test accuracy: 0.9707\n",
      "35 Test accuracy: 0.971\n",
      "36 Test accuracy: 0.9715\n",
      "37 Test accuracy: 0.9723\n",
      "38 Test accuracy: 0.9721\n",
      "39 Test accuracy: 0.9718\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 200\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    # save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "training = tf.placeholder_with_default(False, shape = (), name = 'training')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization, training = training, momentum = batch_norm_momentum)\n",
    "    \n",
    "    my_dense_layer = partial(tf.layers.dense, kernel_initializer = he_init)\n",
    "    \n",
    "    bn1 = my_batch_norm_layer(X)\n",
    "    hidden1 = my_dense_layer(bn1, n_hidden1, activation = tf.nn.elu, name = 'hidden1')\n",
    "    \n",
    "    # bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    bn2 = my_batch_norm_layer(hidden1)\n",
    "    hidden2 = my_dense_layer(bn2, n_hidden2, activation = tf.nn.elu, name = 'hidden2')\n",
    "    \n",
    "    # bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    # logits_before_bn = my_dense_layer(bn2, n_outputs, name = 'outputs')\n",
    "\n",
    "    bn3 = my_batch_norm_layer(hidden2)\n",
    "    logits_before_bn = my_dense_layer(bn3, n_outputs, name = 'outputs')\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test accuracy: 0.8614\n",
      "1 test accuracy: 0.8881\n",
      "2 test accuracy: 0.9036\n",
      "3 test accuracy: 0.9138\n",
      "4 test accuracy: 0.9211\n",
      "5 test accuracy: 0.9255\n",
      "6 test accuracy: 0.9298\n",
      "7 test accuracy: 0.9323\n",
      "8 test accuracy: 0.9355\n",
      "9 test accuracy: 0.9381\n",
      "10 test accuracy: 0.9408\n",
      "11 test accuracy: 0.9426\n",
      "12 test accuracy: 0.9452\n",
      "13 test accuracy: 0.9468\n",
      "14 test accuracy: 0.9484\n",
      "15 test accuracy: 0.9502\n",
      "16 test accuracy: 0.9522\n",
      "17 test accuracy: 0.9532\n",
      "18 test accuracy: 0.954\n",
      "19 test accuracy: 0.9553\n",
      "20 test accuracy: 0.9559\n",
      "21 test accuracy: 0.9573\n",
      "22 test accuracy: 0.9584\n",
      "23 test accuracy: 0.9585\n",
      "24 test accuracy: 0.9576\n",
      "25 test accuracy: 0.9599\n",
      "26 test accuracy: 0.9606\n",
      "27 test accuracy: 0.9615\n",
      "28 test accuracy: 0.9614\n",
      "29 test accuracy: 0.962\n",
      "30 test accuracy: 0.9619\n",
      "31 test accuracy: 0.963\n",
      "32 test accuracy: 0.9629\n",
      "33 test accuracy: 0.9637\n",
      "34 test accuracy: 0.9641\n",
      "35 test accuracy: 0.9642\n",
      "36 test accuracy: 0.9645\n",
      "37 test accuracy: 0.9654\n",
      "38 test accuracy: 0.9661\n",
      "39 test accuracy: 0.966\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict = {training: True, X: X_batch, y: y_batch})\n",
    "            \n",
    "        accuracy_val = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        \n",
    "        print(epoch, \"test accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, name = 'hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.relu, name = 'hidden2')\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation = tf.nn.relu, name = 'hidden3')\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation = tf.nn.relu, name = 'hidden4')\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation = tf.nn.relu, name = 'hidden5')\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name = 'outputs')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer =tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test accuracy: 0.4981\n",
      "1 test accuracy: 0.819\n",
      "2 test accuracy: 0.8852\n",
      "3 test accuracy: 0.9014\n",
      "4 test accuracy: 0.9141\n",
      "5 test accuracy: 0.9226\n",
      "6 test accuracy: 0.9313\n",
      "7 test accuracy: 0.9336\n",
      "8 test accuracy: 0.9367\n",
      "9 test accuracy: 0.9392\n",
      "10 test accuracy: 0.9458\n",
      "11 test accuracy: 0.9485\n",
      "12 test accuracy: 0.9493\n",
      "13 test accuracy: 0.9516\n",
      "14 test accuracy: 0.955\n",
      "15 test accuracy: 0.9542\n",
      "16 test accuracy: 0.9547\n",
      "17 test accuracy: 0.9583\n",
      "18 test accuracy: 0.9593\n",
      "19 test accuracy: 0.9614\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        \n",
    "        accuracy_val = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        \n",
    "        print(epoch, 'test accuracy:', accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph('./my_model_final.ckpt.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros/shape_as_tensor\n",
      "hidden1/bias/Initializer/zeros/Const\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros/shape_as_tensor\n",
      "hidden2/bias/Initializer/zeros/Const\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros/shape_as_tensor\n",
      "hidden3/bias/Initializer/zeros/Const\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros/shape_as_tensor\n",
      "hidden4/bias/Initializer/zeros/Const\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros/shape_as_tensor\n",
      "hidden5/bias/Initializer/zeros/Const\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros/shape_as_tensor\n",
      "outputs/bias/Initializer/zeros/Const\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "train/gradients/Shape\n",
      "train/gradients/grad_ys_0\n",
      "train/gradients/Fill\n",
      "train/gradients/loss/loss_grad/Reshape/shape\n",
      "train/gradients/loss/loss_grad/Reshape\n",
      "train/gradients/loss/loss_grad/Shape\n",
      "train/gradients/loss/loss_grad/Tile\n",
      "train/gradients/loss/loss_grad/Shape_1\n",
      "train/gradients/loss/loss_grad/Shape_2\n",
      "train/gradients/loss/loss_grad/Const\n",
      "train/gradients/loss/loss_grad/Prod\n",
      "train/gradients/loss/loss_grad/Const_1\n",
      "train/gradients/loss/loss_grad/Prod_1\n",
      "train/gradients/loss/loss_grad/Maximum/y\n",
      "train/gradients/loss/loss_grad/Maximum\n",
      "train/gradients/loss/loss_grad/floordiv\n",
      "train/gradients/loss/loss_grad/Cast\n",
      "train/gradients/loss/loss_grad/truediv\n",
      "train/gradients/zeros_like\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "train/gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "train/clip_by_value/Minimum/y\n",
      "train/clip_by_value/Minimum\n",
      "train/clip_by_value/y\n",
      "train/clip_by_value\n",
      "train/clip_by_value_1/Minimum/y\n",
      "train/clip_by_value_1/Minimum\n",
      "train/clip_by_value_1/y\n",
      "train/clip_by_value_1\n",
      "train/clip_by_value_2/Minimum/y\n",
      "train/clip_by_value_2/Minimum\n",
      "train/clip_by_value_2/y\n",
      "train/clip_by_value_2\n",
      "train/clip_by_value_3/Minimum/y\n",
      "train/clip_by_value_3/Minimum\n",
      "train/clip_by_value_3/y\n",
      "train/clip_by_value_3\n",
      "train/clip_by_value_4/Minimum/y\n",
      "train/clip_by_value_4/Minimum\n",
      "train/clip_by_value_4/y\n",
      "train/clip_by_value_4\n",
      "train/clip_by_value_5/Minimum/y\n",
      "train/clip_by_value_5/Minimum\n",
      "train/clip_by_value_5/y\n",
      "train/clip_by_value_5\n",
      "train/clip_by_value_6/Minimum/y\n",
      "train/clip_by_value_6/Minimum\n",
      "train/clip_by_value_6/y\n",
      "train/clip_by_value_6\n",
      "train/clip_by_value_7/Minimum/y\n",
      "train/clip_by_value_7/Minimum\n",
      "train/clip_by_value_7/y\n",
      "train/clip_by_value_7\n",
      "train/clip_by_value_8/Minimum/y\n",
      "train/clip_by_value_8/Minimum\n",
      "train/clip_by_value_8/y\n",
      "train/clip_by_value_8\n",
      "train/clip_by_value_9/Minimum/y\n",
      "train/clip_by_value_9/Minimum\n",
      "train/clip_by_value_9/y\n",
      "train/clip_by_value_9\n",
      "train/clip_by_value_10/Minimum/y\n",
      "train/clip_by_value_10/Minimum\n",
      "train/clip_by_value_10/y\n",
      "train/clip_by_value_10\n",
      "train/clip_by_value_11/Minimum/y\n",
      "train/clip_by_value_11/Minimum\n",
      "train/clip_by_value_11/y\n",
      "train/clip_by_value_11\n",
      "train/GradientDescent/learning_rate\n",
      "train/GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "train/GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/RestoreV2_6/tensor_names\n",
      "save/RestoreV2_6/shape_and_slices\n",
      "save/RestoreV2_6\n",
      "save/RestoreV2_7/tensor_names\n",
      "save/RestoreV2_7/shape_and_slices\n",
      "save/RestoreV2_7\n",
      "save/RestoreV2_8/tensor_names\n",
      "save/RestoreV2_8/shape_and_slices\n",
      "save/RestoreV2_8\n",
      "save/RestoreV2_9/tensor_names\n",
      "save/RestoreV2_9/shape_and_slices\n",
      "save/RestoreV2_9\n",
      "save/RestoreV2_10/tensor_names\n",
      "save/RestoreV2_10/shape_and_slices\n",
      "save/RestoreV2_10\n",
      "save/RestoreV2_11/tensor_names\n",
      "save/RestoreV2_11/shape_and_slices\n",
      "save/RestoreV2_11\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name('eval/accuracy:0')\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name('train/GradientDescent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection('my_important_ops', op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection('my_important_ops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9607\n",
      "1 Test accuracy: 0.9612\n",
      "2 Test accuracy: 0.9633\n",
      "3 Test accuracy: 0.9631\n",
      "4 Test accuracy: 0.9632\n",
      "5 Test accuracy: 0.9653\n",
      "6 Test accuracy: 0.9657\n",
      "7 Test accuracy: 0.9662\n",
      "8 Test accuracy: 0.9671\n",
      "9 Test accuracy: 0.9655\n",
      "10 Test accuracy: 0.968\n",
      "11 Test accuracy: 0.9667\n",
      "12 Test accuracy: 0.9682\n",
      "13 Test accuracy: 0.9664\n",
      "14 Test accuracy: 0.9666\n",
      "15 Test accuracy: 0.9679\n",
      "16 Test accuracy: 0.9695\n",
      "17 Test accuracy: 0.9692\n",
      "18 Test accuracy: 0.97\n",
      "19 Test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './my_model_final.ckpt')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "            \n",
    "        accuracy_val = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, 'Test accuracy:', accuracy_val)\n",
    "    \n",
    "    save_path = saver.save(sess, './my_new_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "saver = tf.train.import_meta_graph('./my_model_final.ckpt.meta')\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name('X:0')\n",
    "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name('dnn/hidden4/Relu:0')\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation = tf.nn.relu, name = 'new_hidden4')\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name = 'new_outputs')\n",
    "\n",
    "with tf.name_scope('new_loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "    \n",
    "with tf.name_scope('new_eval'):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "\n",
    "with tf.name_scope('new_train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 test accuracy: 0.9341\n",
      "1 test accuracy: 0.9481\n",
      "2 test accuracy: 0.9545\n",
      "3 test accuracy: 0.956\n",
      "4 test accuracy: 0.9536\n",
      "5 test accuracy: 0.9602\n",
      "6 test accuracy: 0.96\n",
      "7 test accuracy: 0.9629\n",
      "8 test accuracy: 0.96\n",
      "9 test accuracy: 0.963\n",
      "10 test accuracy: 0.9642\n",
      "11 test accuracy: 0.9659\n",
      "12 test accuracy: 0.9643\n",
      "13 test accuracy: 0.966\n",
      "14 test accuracy: 0.9662\n",
      "15 test accuracy: 0.9666\n",
      "16 test accuracy: 0.9674\n",
      "17 test accuracy: 0.9665\n",
      "18 test accuracy: 0.9659\n",
      "19 test accuracy: 0.9682\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, './my_model_final.ckpt')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        \n",
    "        accuracy_val = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        \n",
    "        print(epoch, 'test accuracy:', accuracy_val)\n",
    "        \n",
    "    save_print = new_saver.save(sess, './my_new_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = \"X\")\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, name = 'hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.relu, name = 'hidden2')\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation = tf.nn.relu, name = 'hidden3')\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation = tf.nn.relu, name = 'hidden4')\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name = 'outputs')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 test accuracy: 0.8984\n",
      "1 test accuracy: 0.9299\n",
      "2 test accuracy: 0.9396\n",
      "3 test accuracy: 0.9452\n",
      "4 test accuracy: 0.9503\n",
      "5 test accuracy: 0.9538\n",
      "6 test accuracy: 0.9559\n",
      "7 test accuracy: 0.956\n",
      "8 test accuracy: 0.9588\n",
      "9 test accuracy: 0.9596\n",
      "10 test accuracy: 0.9599\n",
      "11 test accuracy: 0.959\n",
      "12 test accuracy: 0.9615\n",
      "13 test accuracy: 0.9634\n",
      "14 test accuracy: 0.9633\n",
      "15 test accuracy: 0.9632\n",
      "16 test accuracy: 0.9634\n",
      "17 test accuracy: 0.9646\n",
      "18 test accuracy: 0.9635\n",
      "19 test accuracy: 0.9658\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'hidden[123]')\n",
    "\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, './my_model_final.ckpt')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        \n",
    "        accuracy_val = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        \n",
    "        print(epoch, 'test accuracy:', accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, './my_new_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  61.   83.  105.]]\n"
     ]
    }
   ],
   "source": [
    "original_w = [[1., 2., 3.], [4., 5., 6.]]\n",
    "original_b = [7., 8., 9.]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_inputs], name = 'X')\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, name = 'hidden1')\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name('hidden1/kernel/Assign')\n",
    "assign_bias = graph.get_operation_by_name('hidden1/bias/Assign')\n",
    "\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict = {init_kernel: original_w, init_bias: original_b})\n",
    "    \n",
    "    print(hidden1.eval(feed_dict = {X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  61.   83.  105.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]]\n",
    "original_b = [7., 8., 9.]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, name = 'hidden1')\n",
    "\n",
    "with tf.variable_scope(\"\", default_name = \"\", reuse = True):\n",
    "    hidden1_weights = tf.get_variable('hidden1/kernel')\n",
    "    hidden1_biases = tf.get_variable('hidden1/bias')\n",
    "    \n",
    "original_weights = tf.placeholder(tf.float32, shape = (n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape = (n_hidden1))\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict = {original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict = {original_biases: original_b})\n",
    "    \n",
    "    print(hidden1.eval(feed_dict = {X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'hidden1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name('hidden1/kernel:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape =(None), name = 'y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, name = 'hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.relu, name = 'hidden2')\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation = tf.nn.relu, name = 'hidden3')\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation = tf.nn.relu, name = 'hidden4')\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name = 'outputs')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'hidden[34]|outputs')\n",
    "    training_op = optimizer.minimize(loss, var_list = train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 test accuracy 0.8941\n",
      "1 test accuracy 0.9239\n",
      "2 test accuracy 0.9344\n",
      "3 test accuracy 0.9402\n",
      "4 test accuracy 0.9455\n",
      "5 test accuracy 0.9488\n",
      "6 test accuracy 0.9501\n",
      "7 test accuracy 0.9514\n",
      "8 test accuracy 0.9527\n",
      "9 test accuracy 0.9546\n",
      "10 test accuracy 0.9544\n",
      "11 test accuracy 0.9545\n",
      "12 test accuracy 0.9544\n",
      "13 test accuracy 0.9553\n",
      "14 test accuracy 0.9554\n",
      "15 test accuracy 0.9562\n",
      "16 test accuracy 0.957\n",
      "17 test accuracy 0.9566\n",
      "18 test accuracy 0.9571\n",
      "19 test accuracy 0.9574\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'hidden[123]')\n",
    "\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, './my_model_final.ckpt')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        \n",
    "        accuracy_val = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        \n",
    "        print(epoch, 'test accuracy', accuracy_val)\n",
    "        \n",
    "    save_path = new_saver.save(sess, './my_new_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, name = 'hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.relu, name = 'hidden2')\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation = tf.nn.relu, name = 'hidden3')\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation = tf.nn.relu, name = 'hidden4')\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name = 'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 test accuracy: 0.9137\n",
      "1 test accuracy: 0.9378\n",
      "2 test accuracy: 0.9429\n",
      "3 test accuracy: 0.9468\n",
      "4 test accuracy: 0.9489\n",
      "5 test accuracy: 0.9514\n",
      "6 test accuracy: 0.9508\n",
      "7 test accuracy: 0.9514\n",
      "8 test accuracy: 0.9529\n",
      "9 test accuracy: 0.9545\n",
      "10 test accuracy: 0.9542\n",
      "11 test accuracy: 0.9549\n",
      "12 test accuracy: 0.9542\n",
      "13 test accuracy: 0.9559\n",
      "14 test accuracy: 0.9556\n",
      "15 test accuracy: 0.9561\n",
      "16 test accuracy: 0.9564\n",
      "17 test accuracy: 0.9576\n",
      "18 test accuracy: 0.9562\n",
      "19 test accuracy: 0.9568\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'hidden[123]')\n",
    "\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, './my_model_final.ckpt')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "            \n",
    "        accuracy_val = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, 'test accuracy:', accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, './my_new_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, name = 'hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.relu, name = 'hidden2')\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    \n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation = tf.nn.relu, name = 'hidden3')\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation = tf.nn.relu, name = 'hidden4')\n",
    "    \n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name = 'outputs')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'hidden[123]')\n",
    "\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 test accuracy: 0.9166\n",
      "1 test accuracy: 0.9373\n",
      "2 test accuracy: 0.944\n",
      "3 test accuracy: 0.9464\n",
      "4 test accuracy: 0.9495\n",
      "5 test accuracy: 0.9502\n",
      "6 test accuracy: 0.9508\n",
      "7 test accuracy: 0.9528\n",
      "8 test accuracy: 0.953\n",
      "9 test accuracy: 0.9545\n",
      "10 test accuracy: 0.9553\n",
      "11 test accuracy: 0.955\n",
      "12 test accuracy: 0.9544\n",
      "13 test accuracy: 0.9563\n",
      "14 test accuracy: 0.9559\n",
      "15 test accuracy: 0.9557\n",
      "16 test accuracy: 0.9566\n",
      "17 test accuracy: 0.9571\n",
      "18 test accuracy: 0.9566\n",
      "19 test accuracy: 0.9567\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, './my_model_final.ckpt')\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict = {X: mnist.train.images})\n",
    "    h2_cache_test = sess.run(hidden2, feed_dict = {X: mnist.test.images})\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(mnist.train.num_examples)\n",
    "        \n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(mnist.train.labels[shuffled_idx], n_batches)\n",
    "        \n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict = {hidden2: hidden2_batch, y: y_batch})\n",
    "            \n",
    "        accuracy_val = accuracy.eval(feed_dict = {hidden2: h2_cache_test, y: mnist.test.labels})\n",
    "        print(epoch, 'test accuracy:', accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, './my_new_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum = 0.9, use_nesterov = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate = learning_rate, momentum = 0.9, decay = 0.9, epsilon = 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, name = 'hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.relu, name = 'hidden2')\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name = 'outputs')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1 / 10\n",
    "    global_step = tf.Variable(0, trainable = False, name = 'global_step')\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum = 0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test accuracy: 0.9583\n",
      "1 test accuracy: 0.9724\n",
      "2 test accuracy: 0.9753\n",
      "3 test accuracy: 0.9771\n",
      "4 test accuracy: 0.9803\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "            \n",
    "        accuracy_val = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        \n",
    "        print(epoch, 'test accuracy:', accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, name = 'hidden1')\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name = 'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name('hidden1/kernel:0')\n",
    "W2 = tf.get_default_graph().get_tensor_by_name('outputs/kernel:0')\n",
    "\n",
    "scale = 0.001\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name = 'avg_xentropy')\n",
    "    \n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    \n",
    "    loss = tf.add(base_loss, scale * reg_losses, name = 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1 / 10\n",
    "    global_step = tf.Variable(0, trainable = False, name = 'global_step')\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test accuracy: 0.9\n",
      "1 test accuracy: 0.9034\n",
      "2 test accuracy: 0.8978\n",
      "3 test accuracy: 0.9039\n",
      "4 test accuracy: 0.9075\n",
      "5 test accuracy: 0.911\n",
      "6 test accuracy: 0.916\n",
      "7 test accuracy: 0.9183\n",
      "8 test accuracy: 0.9152\n",
      "9 test accuracy: 0.921\n",
      "10 test accuracy: 0.9258\n",
      "11 test accuracy: 0.9249\n",
      "12 test accuracy: 0.9263\n",
      "13 test accuracy: 0.9307\n",
      "14 test accuracy: 0.9288\n",
      "15 test accuracy: 0.9297\n",
      "16 test accuracy: 0.932\n",
      "17 test accuracy: 0.9328\n",
      "18 test accuracy: 0.9345\n",
      "19 test accuracy: 0.9357\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        \n",
    "        accuracy_val = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        \n",
    "        print(epoch, 'test accuracy:', accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dense_layer = partial(tf.layers.dense, activation = tf.nn.relu, kernel_regularizer = tf.contrib.layers.l2_regularizer(scale))\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name = 'hidden1')\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name = 'hidden2')\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation = None, name = 'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name = 'avg_xentropy')\n",
    "    \n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    \n",
    "    loss = tf.add_n([base_loss] + reg_losses, name = 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test accuracy: 0.8325\n",
      "1 test accuracy: 0.8812\n",
      "2 test accuracy: 0.8956\n",
      "3 test accuracy: 0.9061\n",
      "4 test accuracy: 0.9112\n",
      "5 test accuracy: 0.9159\n",
      "6 test accuracy: 0.918\n",
      "7 test accuracy: 0.9208\n",
      "8 test accuracy: 0.9231\n",
      "9 test accuracy: 0.9254\n",
      "10 test accuracy: 0.9282\n",
      "11 test accuracy: 0.9299\n",
      "12 test accuracy: 0.9352\n",
      "13 test accuracy: 0.9346\n",
      "14 test accuracy: 0.937\n",
      "15 test accuracy: 0.9375\n",
      "16 test accuracy: 0.9389\n",
      "17 test accuracy: 0.9402\n",
      "18 test accuracy: 0.9405\n",
      "19 test accuracy: 0.9421\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        \n",
    "        accuracy_val = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, 'test accuracy:', accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape = (), name = 'training')\n",
    "\n",
    "dropout_rate = 0.5\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training = training)\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation = tf.nn.relu, name = 'hidden1')\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training = training)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation = tf.nn.relu, name = 'hidden2')\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training = training)\n",
    "    \n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name = 'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum = 0.9)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test accuracy: 0.9233\n",
      "1 test accuracy: 0.9426\n",
      "2 test accuracy: 0.9478\n",
      "3 test accuracy: 0.9511\n",
      "4 test accuracy: 0.9564\n",
      "5 test accuracy: 0.9609\n",
      "6 test accuracy: 0.9567\n",
      "7 test accuracy: 0.9626\n",
      "8 test accuracy: 0.9635\n",
      "9 test accuracy: 0.9648\n",
      "10 test accuracy: 0.9631\n",
      "11 test accuracy: 0.9632\n",
      "12 test accuracy: 0.965\n",
      "13 test accuracy: 0.9664\n",
      "14 test accuracy: 0.9666\n",
      "15 test accuracy: 0.9655\n",
      "16 test accuracy: 0.9701\n",
      "17 test accuracy: 0.9704\n",
      "18 test accuracy: 0.9693\n",
      "19 test accuracy: 0.9692\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {training: True, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, 'test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, name = 'hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.relu, name = 'hidden2')\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name = 'outputs')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name('hidden1/kernel:0')\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm = threshold, axes = 1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = tf.get_default_graph().get_tensor_by_name('hidden1/kernel:0')\n",
    "clipped_weights2 = tf.clip_by_norm(weights, clip_norm = threshold, axes = 1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test accuracy: 0.9542\n",
      "1 test accuracy: 0.9645\n",
      "2 test accuracy: 0.9733\n",
      "3 test accuracy: 0.9724\n",
      "4 test accuracy: 0.9751\n",
      "5 test accuracy: 0.9763\n",
      "6 test accuracy: 0.9747\n",
      "7 test accuracy: 0.9801\n",
      "8 test accuracy: 0.9793\n",
      "9 test accuracy: 0.9793\n",
      "10 test accuracy: 0.9797\n",
      "11 test accuracy: 0.9822\n",
      "12 test accuracy: 0.9811\n",
      "13 test accuracy: 0.9809\n",
      "14 test accuracy: 0.9815\n",
      "15 test accuracy: 0.9818\n",
      "16 test accuracy: 0.9817\n",
      "17 test accuracy: 0.9819\n",
      "18 test accuracy: 0.9816\n",
      "19 test accuracy: 0.9821\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()\n",
    "            \n",
    "        acc_test = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, 'test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes = 1, name = 'max_norm', collection = 'max_norm'):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm = threshold, axes = axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name = name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name =\"X\")\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold = 1.0)\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, kernel_regularizer = max_norm_reg, name = 'hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.relu, kernel_regularizer = max_norm_reg, name = 'hidden2')\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name = 'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = y)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test accuracy: 0.9514\n",
      "1 test accuracy: 0.9674\n",
      "2 test accuracy: 0.9712\n",
      "3 test accuracy: 0.973\n",
      "4 test accuracy: 0.9768\n",
      "5 test accuracy: 0.9715\n",
      "6 test accuracy: 0.9769\n",
      "7 test accuracy: 0.9769\n",
      "8 test accuracy: 0.9798\n",
      "9 test accuracy: 0.9766\n",
      "10 test accuracy: 0.9793\n",
      "11 test accuracy: 0.9793\n",
      "12 test accuracy: 0.9799\n",
      "13 test accuracy: 0.9813\n",
      "14 test accuracy: 0.9804\n",
      "15 test accuracy: 0.9809\n",
      "16 test accuracy: 0.9819\n",
      "17 test accuracy: 0.9807\n",
      "18 test accuracy: 0.9809\n",
      "19 test accuracy: 0.9813\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "clip_all_weights = tf.get_collection('max_norm')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "\n",
    "        acc_test = accuracy.eval(feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, 'test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers = 5, n_neurons = 100, name = None, activation = tf.nn.elu, initializer = he_init):\n",
    "    with tf.variable_scope(name, 'dnn'):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation = activation, kernel_initializer = initializer, name = 'hidden%d' %(layer + 1))\n",
    "            \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_outputs = 5\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "\n",
    "dnn_outputs = dnn(X)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer = he_init, name = 'logits')\n",
    "Y_prob = tf.nn.softmax(logits, name = 'Y_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss, name = 'training_op')\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.148721\tBest loss: 0.148721\tAccuracy: 96.87%\n",
      "1\tValidation loss: 0.080501\tBest loss: 0.080501\tAccuracy: 98.05%\n",
      "2\tValidation loss: 0.105855\tBest loss: 0.080501\tAccuracy: 97.38%\n",
      "3\tValidation loss: 0.130847\tBest loss: 0.080501\tAccuracy: 97.54%\n",
      "4\tValidation loss: 0.170205\tBest loss: 0.080501\tAccuracy: 96.29%\n",
      "5\tValidation loss: 0.197716\tBest loss: 0.080501\tAccuracy: 96.36%\n",
      "6\tValidation loss: 0.134526\tBest loss: 0.080501\tAccuracy: 97.77%\n",
      "7\tValidation loss: 0.129417\tBest loss: 0.080501\tAccuracy: 98.01%\n",
      "8\tValidation loss: 0.274585\tBest loss: 0.080501\tAccuracy: 94.84%\n",
      "9\tValidation loss: 0.450857\tBest loss: 0.080501\tAccuracy: 93.55%\n",
      "10\tValidation loss: 0.687849\tBest loss: 0.080501\tAccuracy: 89.76%\n",
      "11\tValidation loss: 0.180299\tBest loss: 0.080501\tAccuracy: 96.95%\n",
      "12\tValidation loss: 0.813346\tBest loss: 0.080501\tAccuracy: 71.03%\n",
      "13\tValidation loss: 0.922601\tBest loss: 0.080501\tAccuracy: 74.78%\n",
      "14\tValidation loss: 1.020081\tBest loss: 0.080501\tAccuracy: 56.72%\n",
      "15\tValidation loss: 0.475778\tBest loss: 0.080501\tAccuracy: 87.26%\n",
      "16\tValidation loss: 0.456737\tBest loss: 0.080501\tAccuracy: 91.24%\n",
      "17\tValidation loss: 0.328601\tBest loss: 0.080501\tAccuracy: 96.13%\n",
      "18\tValidation loss: 0.302504\tBest loss: 0.080501\tAccuracy: 97.50%\n",
      "19\tValidation loss: 1.028939\tBest loss: 0.080501\tAccuracy: 77.40%\n",
      "20\tValidation loss: 0.927926\tBest loss: 0.080501\tAccuracy: 58.13%\n",
      "21\tValidation loss: 0.481138\tBest loss: 0.080501\tAccuracy: 97.85%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt\n",
      "final test accuracy: 97.96%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_check_without_progress = 20\n",
    "check_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "            \n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict = {X: X_valid1, y: y_valid1})\n",
    "        \n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, './my_mnist_model_0_to_4.ckpt')\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_check_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "                \n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './my_mnist_model_0_to_4.ckpt')\n",
    "    acc_test = accuracy.eval(feed_dict = {X: X_test1, y: y_test1})\n",
    "    \n",
    "    print('final test accuracy: {:.2f}%'.format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers = 5, n_neurons = 100, optimizer_class = tf.train.AdamOptimizer,\n",
    "                learning_rate = 0.01, batch_size = 20, activation = tf.nn.elu, initializer = he_init,\n",
    "                batch_norm_momentum = None, dropout_rate = None, random_state = None):\n",
    "        \n",
    "        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "        \n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.droupout(inputs, self.dropout_rate, training = self._training)\n",
    "            \n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons, kernel_initializer = self.initializer, name = 'hidden%d' %(layer + 1))\n",
    "            \n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum = self.batch_norm_momentum, training = self._training)\n",
    "                \n",
    "            inputs = self.activation(inputs, name = 'hidden%d_out' %(layer + 1))\n",
    "            \n",
    "        return inputs\n",
    "    \n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the modle\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "        X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "        y = tf.placeholder(tf.int32, shape = (None), name = 'y')\n",
    "        \n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape = (), name = 'training')\n",
    "        else:\n",
    "            self._training = None\n",
    "            \n",
    "        dnn_outputs = self._dnn(X)\n",
    "        \n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer = he_init, name = 'logits')\n",
    "        Y_proba = tf.nn.softmax(logits, name = 'Y_proba')\n",
    "        \n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "        loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "        \n",
    "        optimizer = self.optimizer_class(learning_rate = self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "        \n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "        \n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "            \n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "            \n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + '/Assign') for gvar_name in gvar_names}\n",
    "        \n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        \n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict = feed_dict)\n",
    "        \n",
    "    def fit(self, X, y, n_epochs = 1000, X_valid = None, y_valid = None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "        \n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        self.class_to_index_ = {label: index for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label] for label in y], dtype = np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            \n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        self._session = tf.Session(graph = self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            \n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    \n",
    "                    sess.run(self._training_op, feed_dict = feed_dict)\n",
    "                    \n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict = feed_dict)\n",
    "                    \n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy], feed_dict = {self._X: X_valid, self._y: y_valid})\n",
    "                        \n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                        \n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                    epoch, loss_val, best_loss, acc_val * 100))\n",
    "                        \n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping\")\n",
    "                        break\n",
    "                            \n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy], feed_dict = {self._X: X_batch, self._y: y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                    epoch, loss_train, acc_train * 100))\n",
    "                    \n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "                \n",
    "            return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" %self.__class__.__name__)\n",
    "        \n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict = {self._X: X})\n",
    "        \n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis = 1)\n",
    "        return np.array([[self.classes_[class_index]] for class_index in class_indices], np.int32)\n",
    "    \n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "\n",
    "        dnn_outputs = self._dnn(X)\n",
    "\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                                  logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Make the important operations available easily through instance variables\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Translate the labels vector to a vector of sorted class indices, containing\n",
    "        # integers from 0 to n_outputs - 1.\n",
    "        # For example, if y is equal to [8, 8, 9, 5, 7, 6, 6, 6], then the sorted class\n",
    "        # labels (self.classes_) will be equal to [5, 6, 7, 8, 9], and the labels vector\n",
    "        # will be translated to [3, 3, 4, 0, 2, 1, 1, 1]\n",
    "        self.class_to_index_ = {label: index\n",
    "                                for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # extra ops for batch normalization\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        # needed in case of early stopping\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        # Now train the model!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch,\n",
    "                                                                self._y: y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # If we used early stopping then rollback to the best model found\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.284343\tBest loss: 0.284343\tAccuracy: 91.24%\n",
      "1\tValidation loss: 0.116198\tBest loss: 0.116198\tAccuracy: 97.07%\n",
      "2\tValidation loss: 0.193899\tBest loss: 0.116198\tAccuracy: 96.25%\n",
      "3\tValidation loss: 0.375284\tBest loss: 0.116198\tAccuracy: 97.26%\n",
      "4\tValidation loss: 0.181877\tBest loss: 0.116198\tAccuracy: 96.68%\n",
      "5\tValidation loss: 0.132137\tBest loss: 0.116198\tAccuracy: 97.11%\n",
      "6\tValidation loss: 1.960354\tBest loss: 0.116198\tAccuracy: 76.66%\n",
      "7\tValidation loss: 0.246118\tBest loss: 0.116198\tAccuracy: 96.56%\n",
      "8\tValidation loss: 0.131025\tBest loss: 0.116198\tAccuracy: 97.54%\n",
      "9\tValidation loss: 0.308136\tBest loss: 0.116198\tAccuracy: 95.19%\n",
      "10\tValidation loss: 0.157841\tBest loss: 0.116198\tAccuracy: 97.26%\n",
      "11\tValidation loss: 0.169729\tBest loss: 0.116198\tAccuracy: 97.03%\n",
      "12\tValidation loss: 0.838201\tBest loss: 0.116198\tAccuracy: 85.54%\n",
      "13\tValidation loss: 0.206869\tBest loss: 0.116198\tAccuracy: 95.66%\n",
      "14\tValidation loss: 0.242623\tBest loss: 0.116198\tAccuracy: 97.07%\n",
      "15\tValidation loss: 0.215754\tBest loss: 0.116198\tAccuracy: 96.21%\n",
      "16\tValidation loss: 0.125613\tBest loss: 0.116198\tAccuracy: 98.05%\n",
      "17\tValidation loss: 0.182634\tBest loss: 0.116198\tAccuracy: 97.85%\n",
      "18\tValidation loss: 0.186583\tBest loss: 0.116198\tAccuracy: 97.97%\n",
      "19\tValidation loss: 0.220369\tBest loss: 0.116198\tAccuracy: 97.19%\n",
      "20\tValidation loss: 0.353908\tBest loss: 0.116198\tAccuracy: 95.19%\n",
      "21\tValidation loss: 0.149040\tBest loss: 0.116198\tAccuracy: 97.46%\n",
      "22\tValidation loss: 1.504938\tBest loss: 0.116198\tAccuracy: 56.53%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function elu at 0x11b8a9d08>,\n",
       "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x122b289d8>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = DNNClassifier(random_state = 42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs = 1000, X_valid = X_valid1, y_valid = y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97450865927223196"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lipenghua/anaconda2/envs/ml/lib/python3.5/site-packages/sklearn/model_selection/_search.py:584: DeprecationWarning: \"fit_params\" as a constructor argument was deprecated in version 0.19 and will be removed in version 0.21. Pass fit parameters to the \"fit\" method instead.\n",
      "  '\"fit\" method instead.', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV] learning_rate=0.05, activation=<function elu at 0x11b8a9d08>, batch_size=100, n_neurons=10 \n",
      "0\tValidation loss: 0.166256\tBest loss: 0.166256\tAccuracy: 95.54%\n",
      "1\tValidation loss: 0.146161\tBest loss: 0.146161\tAccuracy: 96.44%\n",
      "2\tValidation loss: 0.122708\tBest loss: 0.122708\tAccuracy: 96.91%\n",
      "3\tValidation loss: 0.122841\tBest loss: 0.122708\tAccuracy: 96.95%\n",
      "4\tValidation loss: 0.182096\tBest loss: 0.122708\tAccuracy: 95.47%\n",
      "5\tValidation loss: 0.244316\tBest loss: 0.122708\tAccuracy: 95.31%\n",
      "6\tValidation loss: 1.203549\tBest loss: 0.122708\tAccuracy: 41.48%\n",
      "7\tValidation loss: 1.314586\tBest loss: 0.122708\tAccuracy: 42.69%\n",
      "8\tValidation loss: 1.176271\tBest loss: 0.122708\tAccuracy: 39.25%\n",
      "9\tValidation loss: 1.163501\tBest loss: 0.122708\tAccuracy: 42.10%\n",
      "10\tValidation loss: 1.188630\tBest loss: 0.122708\tAccuracy: 41.56%\n",
      "11\tValidation loss: 1.159671\tBest loss: 0.122708\tAccuracy: 39.01%\n",
      "12\tValidation loss: 1.165159\tBest loss: 0.122708\tAccuracy: 42.46%\n",
      "13\tValidation loss: 1.175433\tBest loss: 0.122708\tAccuracy: 39.48%\n",
      "14\tValidation loss: 1.160384\tBest loss: 0.122708\tAccuracy: 42.73%\n",
      "15\tValidation loss: 1.156941\tBest loss: 0.122708\tAccuracy: 39.25%\n",
      "16\tValidation loss: 1.167503\tBest loss: 0.122708\tAccuracy: 42.26%\n",
      "17\tValidation loss: 1.174700\tBest loss: 0.122708\tAccuracy: 39.76%\n",
      "18\tValidation loss: 1.201646\tBest loss: 0.122708\tAccuracy: 42.73%\n",
      "19\tValidation loss: 1.173684\tBest loss: 0.122708\tAccuracy: 38.94%\n",
      "20\tValidation loss: 1.178429\tBest loss: 0.122708\tAccuracy: 42.53%\n",
      "21\tValidation loss: 1.157199\tBest loss: 0.122708\tAccuracy: 39.76%\n",
      "22\tValidation loss: 1.181557\tBest loss: 0.122708\tAccuracy: 38.94%\n",
      "23\tValidation loss: 1.157898\tBest loss: 0.122708\tAccuracy: 39.68%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.05, activation=<function elu at 0x11b8a9d08>, batch_size=100, n_neurons=10, total=   5.3s\n",
      "[CV] learning_rate=0.05, activation=<function elu at 0x11b8a9d08>, batch_size=100, n_neurons=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.215637\tBest loss: 0.215637\tAccuracy: 93.82%\n",
      "1\tValidation loss: 0.139495\tBest loss: 0.139495\tAccuracy: 96.56%\n",
      "2\tValidation loss: 0.159821\tBest loss: 0.139495\tAccuracy: 95.82%\n",
      "3\tValidation loss: 0.131446\tBest loss: 0.131446\tAccuracy: 97.11%\n",
      "4\tValidation loss: 0.167358\tBest loss: 0.131446\tAccuracy: 96.09%\n",
      "5\tValidation loss: 0.102949\tBest loss: 0.102949\tAccuracy: 97.62%\n",
      "6\tValidation loss: 0.098427\tBest loss: 0.098427\tAccuracy: 97.15%\n",
      "7\tValidation loss: 0.492749\tBest loss: 0.098427\tAccuracy: 84.56%\n",
      "8\tValidation loss: 1.197615\tBest loss: 0.098427\tAccuracy: 40.34%\n",
      "9\tValidation loss: 1.063114\tBest loss: 0.098427\tAccuracy: 40.58%\n",
      "10\tValidation loss: 1.053660\tBest loss: 0.098427\tAccuracy: 37.69%\n",
      "11\tValidation loss: 1.046094\tBest loss: 0.098427\tAccuracy: 39.91%\n",
      "12\tValidation loss: 1.337075\tBest loss: 0.098427\tAccuracy: 36.51%\n",
      "13\tValidation loss: 1.220009\tBest loss: 0.098427\tAccuracy: 37.61%\n",
      "14\tValidation loss: 1.144782\tBest loss: 0.098427\tAccuracy: 40.66%\n",
      "15\tValidation loss: 1.124602\tBest loss: 0.098427\tAccuracy: 39.76%\n",
      "16\tValidation loss: 1.084473\tBest loss: 0.098427\tAccuracy: 40.34%\n",
      "17\tValidation loss: 1.059233\tBest loss: 0.098427\tAccuracy: 37.10%\n",
      "18\tValidation loss: 1.047375\tBest loss: 0.098427\tAccuracy: 40.62%\n",
      "19\tValidation loss: 1.101892\tBest loss: 0.098427\tAccuracy: 42.53%\n",
      "20\tValidation loss: 1.083843\tBest loss: 0.098427\tAccuracy: 40.54%\n",
      "21\tValidation loss: 1.178065\tBest loss: 0.098427\tAccuracy: 39.29%\n",
      "22\tValidation loss: 1.121804\tBest loss: 0.098427\tAccuracy: 42.42%\n",
      "23\tValidation loss: 1.069060\tBest loss: 0.098427\tAccuracy: 40.58%\n",
      "24\tValidation loss: 1.068952\tBest loss: 0.098427\tAccuracy: 40.66%\n",
      "25\tValidation loss: 1.058092\tBest loss: 0.098427\tAccuracy: 37.41%\n",
      "26\tValidation loss: 1.079149\tBest loss: 0.098427\tAccuracy: 40.58%\n",
      "27\tValidation loss: 1.052391\tBest loss: 0.098427\tAccuracy: 37.76%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.05, activation=<function elu at 0x11b8a9d08>, batch_size=100, n_neurons=10, total=   5.7s\n",
      "[CV] learning_rate=0.05, activation=<function elu at 0x11b8a9d08>, batch_size=100, n_neurons=10 \n",
      "0\tValidation loss: 0.153529\tBest loss: 0.153529\tAccuracy: 95.86%\n",
      "1\tValidation loss: 0.127054\tBest loss: 0.127054\tAccuracy: 96.68%\n",
      "2\tValidation loss: 0.138129\tBest loss: 0.127054\tAccuracy: 96.56%\n",
      "3\tValidation loss: 0.137710\tBest loss: 0.127054\tAccuracy: 96.52%\n",
      "4\tValidation loss: 0.142417\tBest loss: 0.127054\tAccuracy: 96.13%\n",
      "5\tValidation loss: 0.122720\tBest loss: 0.122720\tAccuracy: 97.03%\n",
      "6\tValidation loss: 0.225426\tBest loss: 0.122720\tAccuracy: 94.92%\n",
      "7\tValidation loss: 1.052769\tBest loss: 0.122720\tAccuracy: 37.61%\n",
      "8\tValidation loss: 0.812454\tBest loss: 0.122720\tAccuracy: 60.75%\n",
      "9\tValidation loss: 1.236097\tBest loss: 0.122720\tAccuracy: 38.27%\n",
      "10\tValidation loss: 1.179407\tBest loss: 0.122720\tAccuracy: 42.14%\n",
      "11\tValidation loss: 1.183252\tBest loss: 0.122720\tAccuracy: 39.44%\n",
      "12\tValidation loss: 1.170280\tBest loss: 0.122720\tAccuracy: 39.91%\n",
      "13\tValidation loss: 1.149105\tBest loss: 0.122720\tAccuracy: 42.18%\n",
      "14\tValidation loss: 1.643324\tBest loss: 0.122720\tAccuracy: 18.73%\n",
      "15\tValidation loss: 1.634119\tBest loss: 0.122720\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.638207\tBest loss: 0.122720\tAccuracy: 18.73%\n",
      "17\tValidation loss: 1.636636\tBest loss: 0.122720\tAccuracy: 19.08%\n",
      "18\tValidation loss: 1.612617\tBest loss: 0.122720\tAccuracy: 20.91%\n",
      "19\tValidation loss: 1.617661\tBest loss: 0.122720\tAccuracy: 19.08%\n",
      "20\tValidation loss: 1.616046\tBest loss: 0.122720\tAccuracy: 22.01%\n",
      "21\tValidation loss: 1.624680\tBest loss: 0.122720\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.621631\tBest loss: 0.122720\tAccuracy: 22.01%\n",
      "23\tValidation loss: 1.623731\tBest loss: 0.122720\tAccuracy: 22.01%\n",
      "24\tValidation loss: 1.609585\tBest loss: 0.122720\tAccuracy: 20.91%\n",
      "25\tValidation loss: 1.620248\tBest loss: 0.122720\tAccuracy: 19.27%\n",
      "26\tValidation loss: 1.613711\tBest loss: 0.122720\tAccuracy: 18.73%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.05, activation=<function elu at 0x11b8a9d08>, batch_size=100, n_neurons=10, total=   5.4s\n",
      "[CV] learning_rate=0.02, activation=<function relu at 0x11b8d3ae8>, batch_size=500, n_neurons=30 \n",
      "0\tValidation loss: 0.133888\tBest loss: 0.133888\tAccuracy: 95.74%\n",
      "1\tValidation loss: 0.083944\tBest loss: 0.083944\tAccuracy: 97.54%\n",
      "2\tValidation loss: 0.079635\tBest loss: 0.079635\tAccuracy: 97.30%\n",
      "3\tValidation loss: 0.083644\tBest loss: 0.079635\tAccuracy: 97.65%\n",
      "4\tValidation loss: 0.078301\tBest loss: 0.078301\tAccuracy: 97.73%\n",
      "5\tValidation loss: 0.079670\tBest loss: 0.078301\tAccuracy: 97.85%\n",
      "6\tValidation loss: 0.068624\tBest loss: 0.068624\tAccuracy: 98.28%\n",
      "7\tValidation loss: 0.099379\tBest loss: 0.068624\tAccuracy: 97.81%\n",
      "8\tValidation loss: 0.068018\tBest loss: 0.068018\tAccuracy: 98.32%\n",
      "9\tValidation loss: 0.064400\tBest loss: 0.064400\tAccuracy: 98.40%\n",
      "10\tValidation loss: 0.067059\tBest loss: 0.064400\tAccuracy: 98.24%\n",
      "11\tValidation loss: 0.083986\tBest loss: 0.064400\tAccuracy: 98.44%\n",
      "12\tValidation loss: 0.078897\tBest loss: 0.064400\tAccuracy: 98.32%\n",
      "13\tValidation loss: 0.077020\tBest loss: 0.064400\tAccuracy: 98.24%\n",
      "14\tValidation loss: 0.079263\tBest loss: 0.064400\tAccuracy: 98.20%\n",
      "15\tValidation loss: 0.091174\tBest loss: 0.064400\tAccuracy: 98.05%\n",
      "16\tValidation loss: 0.085167\tBest loss: 0.064400\tAccuracy: 98.08%\n",
      "17\tValidation loss: 0.081824\tBest loss: 0.064400\tAccuracy: 98.40%\n",
      "18\tValidation loss: 0.113968\tBest loss: 0.064400\tAccuracy: 97.85%\n",
      "19\tValidation loss: 0.077129\tBest loss: 0.064400\tAccuracy: 98.36%\n",
      "20\tValidation loss: 0.093626\tBest loss: 0.064400\tAccuracy: 98.12%\n",
      "21\tValidation loss: 0.101567\tBest loss: 0.064400\tAccuracy: 98.12%\n",
      "22\tValidation loss: 0.088186\tBest loss: 0.064400\tAccuracy: 98.32%\n",
      "23\tValidation loss: 0.090712\tBest loss: 0.064400\tAccuracy: 98.40%\n",
      "24\tValidation loss: 0.081703\tBest loss: 0.064400\tAccuracy: 98.51%\n",
      "25\tValidation loss: 0.095919\tBest loss: 0.064400\tAccuracy: 98.01%\n",
      "26\tValidation loss: 0.085827\tBest loss: 0.064400\tAccuracy: 98.20%\n",
      "27\tValidation loss: 0.106648\tBest loss: 0.064400\tAccuracy: 97.77%\n",
      "28\tValidation loss: 0.083833\tBest loss: 0.064400\tAccuracy: 98.36%\n",
      "29\tValidation loss: 0.089252\tBest loss: 0.064400\tAccuracy: 98.51%\n",
      "30\tValidation loss: 0.116259\tBest loss: 0.064400\tAccuracy: 98.24%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.02, activation=<function relu at 0x11b8d3ae8>, batch_size=500, n_neurons=30, total=   4.3s\n",
      "[CV] learning_rate=0.02, activation=<function relu at 0x11b8d3ae8>, batch_size=500, n_neurons=30 \n",
      "0\tValidation loss: 0.107289\tBest loss: 0.107289\tAccuracy: 96.87%\n",
      "1\tValidation loss: 0.085173\tBest loss: 0.085173\tAccuracy: 97.58%\n",
      "2\tValidation loss: 0.070480\tBest loss: 0.070480\tAccuracy: 98.01%\n",
      "3\tValidation loss: 0.068425\tBest loss: 0.068425\tAccuracy: 98.08%\n",
      "4\tValidation loss: 0.076632\tBest loss: 0.068425\tAccuracy: 98.16%\n",
      "5\tValidation loss: 0.078700\tBest loss: 0.068425\tAccuracy: 97.81%\n",
      "6\tValidation loss: 0.073676\tBest loss: 0.068425\tAccuracy: 98.05%\n",
      "7\tValidation loss: 0.066256\tBest loss: 0.066256\tAccuracy: 98.32%\n",
      "8\tValidation loss: 0.071906\tBest loss: 0.066256\tAccuracy: 98.12%\n",
      "9\tValidation loss: 0.070456\tBest loss: 0.066256\tAccuracy: 98.12%\n",
      "10\tValidation loss: 0.061487\tBest loss: 0.061487\tAccuracy: 98.28%\n",
      "11\tValidation loss: 0.064647\tBest loss: 0.061487\tAccuracy: 98.12%\n",
      "12\tValidation loss: 0.119023\tBest loss: 0.061487\tAccuracy: 97.19%\n",
      "13\tValidation loss: 0.068631\tBest loss: 0.061487\tAccuracy: 98.20%\n",
      "14\tValidation loss: 0.078439\tBest loss: 0.061487\tAccuracy: 97.89%\n",
      "15\tValidation loss: 0.072101\tBest loss: 0.061487\tAccuracy: 98.44%\n",
      "16\tValidation loss: 0.069915\tBest loss: 0.061487\tAccuracy: 98.24%\n",
      "17\tValidation loss: 0.072254\tBest loss: 0.061487\tAccuracy: 98.16%\n",
      "18\tValidation loss: 0.082804\tBest loss: 0.061487\tAccuracy: 98.36%\n",
      "19\tValidation loss: 0.087102\tBest loss: 0.061487\tAccuracy: 98.12%\n",
      "20\tValidation loss: 0.078541\tBest loss: 0.061487\tAccuracy: 98.16%\n",
      "21\tValidation loss: 0.089423\tBest loss: 0.061487\tAccuracy: 98.05%\n",
      "22\tValidation loss: 0.092708\tBest loss: 0.061487\tAccuracy: 98.16%\n",
      "23\tValidation loss: 0.090446\tBest loss: 0.061487\tAccuracy: 98.24%\n",
      "24\tValidation loss: 0.089653\tBest loss: 0.061487\tAccuracy: 98.24%\n",
      "25\tValidation loss: 0.083498\tBest loss: 0.061487\tAccuracy: 98.36%\n",
      "26\tValidation loss: 0.126762\tBest loss: 0.061487\tAccuracy: 97.93%\n",
      "27\tValidation loss: 0.091915\tBest loss: 0.061487\tAccuracy: 98.28%\n",
      "28\tValidation loss: 0.149076\tBest loss: 0.061487\tAccuracy: 97.65%\n",
      "29\tValidation loss: 0.093960\tBest loss: 0.061487\tAccuracy: 98.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\tValidation loss: 0.099091\tBest loss: 0.061487\tAccuracy: 98.20%\n",
      "31\tValidation loss: 0.115697\tBest loss: 0.061487\tAccuracy: 98.05%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.02, activation=<function relu at 0x11b8d3ae8>, batch_size=500, n_neurons=30, total=   4.2s\n",
      "[CV] learning_rate=0.02, activation=<function relu at 0x11b8d3ae8>, batch_size=500, n_neurons=30 \n",
      "0\tValidation loss: 0.121246\tBest loss: 0.121246\tAccuracy: 96.33%\n",
      "1\tValidation loss: 0.088884\tBest loss: 0.088884\tAccuracy: 97.46%\n",
      "2\tValidation loss: 0.084427\tBest loss: 0.084427\tAccuracy: 97.50%\n",
      "3\tValidation loss: 0.077792\tBest loss: 0.077792\tAccuracy: 97.97%\n",
      "4\tValidation loss: 0.068139\tBest loss: 0.068139\tAccuracy: 98.05%\n",
      "5\tValidation loss: 0.064043\tBest loss: 0.064043\tAccuracy: 98.36%\n",
      "6\tValidation loss: 0.072755\tBest loss: 0.064043\tAccuracy: 98.24%\n",
      "7\tValidation loss: 0.100934\tBest loss: 0.064043\tAccuracy: 97.54%\n",
      "8\tValidation loss: 0.067828\tBest loss: 0.064043\tAccuracy: 98.12%\n",
      "9\tValidation loss: 0.081844\tBest loss: 0.064043\tAccuracy: 98.12%\n",
      "10\tValidation loss: 0.080018\tBest loss: 0.064043\tAccuracy: 98.05%\n",
      "11\tValidation loss: 0.062618\tBest loss: 0.062618\tAccuracy: 98.40%\n",
      "12\tValidation loss: 0.064632\tBest loss: 0.062618\tAccuracy: 98.55%\n",
      "13\tValidation loss: 0.071942\tBest loss: 0.062618\tAccuracy: 98.40%\n",
      "14\tValidation loss: 0.076456\tBest loss: 0.062618\tAccuracy: 98.44%\n",
      "15\tValidation loss: 0.081090\tBest loss: 0.062618\tAccuracy: 98.40%\n",
      "16\tValidation loss: 0.070865\tBest loss: 0.062618\tAccuracy: 98.67%\n",
      "17\tValidation loss: 0.070312\tBest loss: 0.062618\tAccuracy: 98.75%\n",
      "18\tValidation loss: 0.066814\tBest loss: 0.062618\tAccuracy: 98.71%\n",
      "19\tValidation loss: 0.078473\tBest loss: 0.062618\tAccuracy: 98.28%\n",
      "20\tValidation loss: 0.091115\tBest loss: 0.062618\tAccuracy: 98.05%\n",
      "21\tValidation loss: 0.068706\tBest loss: 0.062618\tAccuracy: 98.59%\n",
      "22\tValidation loss: 0.087246\tBest loss: 0.062618\tAccuracy: 98.40%\n",
      "23\tValidation loss: 0.068909\tBest loss: 0.062618\tAccuracy: 98.51%\n",
      "24\tValidation loss: 0.067567\tBest loss: 0.062618\tAccuracy: 98.40%\n",
      "25\tValidation loss: 0.087663\tBest loss: 0.062618\tAccuracy: 97.93%\n",
      "26\tValidation loss: 0.123602\tBest loss: 0.062618\tAccuracy: 98.08%\n",
      "27\tValidation loss: 0.066826\tBest loss: 0.062618\tAccuracy: 98.55%\n",
      "28\tValidation loss: 0.082751\tBest loss: 0.062618\tAccuracy: 98.28%\n",
      "29\tValidation loss: 0.107585\tBest loss: 0.062618\tAccuracy: 98.44%\n",
      "30\tValidation loss: 0.072912\tBest loss: 0.062618\tAccuracy: 98.63%\n",
      "31\tValidation loss: 0.089359\tBest loss: 0.062618\tAccuracy: 98.44%\n",
      "32\tValidation loss: 0.066992\tBest loss: 0.062618\tAccuracy: 98.79%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.02, activation=<function relu at 0x11b8d3ae8>, batch_size=500, n_neurons=30, total=   4.1s\n",
      "[CV] learning_rate=0.05, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=50, n_neurons=90 \n",
      "0\tValidation loss: 0.861714\tBest loss: 0.861714\tAccuracy: 72.36%\n",
      "1\tValidation loss: 0.637672\tBest loss: 0.637672\tAccuracy: 78.11%\n",
      "2\tValidation loss: 0.424853\tBest loss: 0.424853\tAccuracy: 87.57%\n",
      "3\tValidation loss: 34059.097656\tBest loss: 0.424853\tAccuracy: 19.08%\n",
      "4\tValidation loss: 240.637222\tBest loss: 0.424853\tAccuracy: 44.80%\n",
      "5\tValidation loss: 187.870270\tBest loss: 0.424853\tAccuracy: 35.14%\n",
      "6\tValidation loss: 89.038742\tBest loss: 0.424853\tAccuracy: 50.59%\n",
      "7\tValidation loss: 27.988476\tBest loss: 0.424853\tAccuracy: 70.60%\n",
      "8\tValidation loss: 124.129494\tBest loss: 0.424853\tAccuracy: 54.03%\n",
      "9\tValidation loss: 43.311642\tBest loss: 0.424853\tAccuracy: 62.20%\n",
      "10\tValidation loss: 84.122375\tBest loss: 0.424853\tAccuracy: 54.03%\n",
      "11\tValidation loss: 25.153482\tBest loss: 0.424853\tAccuracy: 76.23%\n",
      "12\tValidation loss: 1733.885620\tBest loss: 0.424853\tAccuracy: 22.36%\n",
      "13\tValidation loss: 455.832886\tBest loss: 0.424853\tAccuracy: 33.85%\n",
      "14\tValidation loss: 53.861507\tBest loss: 0.424853\tAccuracy: 50.70%\n",
      "15\tValidation loss: 35.462540\tBest loss: 0.424853\tAccuracy: 49.41%\n",
      "16\tValidation loss: 71.825203\tBest loss: 0.424853\tAccuracy: 38.35%\n",
      "17\tValidation loss: 43.159943\tBest loss: 0.424853\tAccuracy: 49.96%\n",
      "18\tValidation loss: 26.196657\tBest loss: 0.424853\tAccuracy: 55.79%\n",
      "19\tValidation loss: 708.552795\tBest loss: 0.424853\tAccuracy: 37.57%\n",
      "20\tValidation loss: 32.412464\tBest loss: 0.424853\tAccuracy: 51.80%\n",
      "21\tValidation loss: 13.806338\tBest loss: 0.424853\tAccuracy: 65.68%\n",
      "22\tValidation loss: 5719936.000000\tBest loss: 0.424853\tAccuracy: 19.08%\n",
      "23\tValidation loss: 2454.528809\tBest loss: 0.424853\tAccuracy: 28.15%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.05, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=50, n_neurons=90, total=  16.1s\n",
      "[CV] learning_rate=0.05, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=50, n_neurons=90 \n",
      "0\tValidation loss: 1.757945\tBest loss: 1.757945\tAccuracy: 52.50%\n",
      "1\tValidation loss: 0.852434\tBest loss: 0.852434\tAccuracy: 68.92%\n",
      "2\tValidation loss: 1.124712\tBest loss: 0.852434\tAccuracy: 66.61%\n",
      "3\tValidation loss: 0.424284\tBest loss: 0.424284\tAccuracy: 86.94%\n",
      "4\tValidation loss: 0.321637\tBest loss: 0.321637\tAccuracy: 90.70%\n",
      "5\tValidation loss: 0.542185\tBest loss: 0.321637\tAccuracy: 84.40%\n",
      "6\tValidation loss: 0.232287\tBest loss: 0.232287\tAccuracy: 92.73%\n",
      "7\tValidation loss: 0.735195\tBest loss: 0.232287\tAccuracy: 81.55%\n",
      "8\tValidation loss: 0.218621\tBest loss: 0.218621\tAccuracy: 94.06%\n",
      "9\tValidation loss: 3574.593506\tBest loss: 0.218621\tAccuracy: 38.51%\n",
      "10\tValidation loss: 2894.760742\tBest loss: 0.218621\tAccuracy: 41.28%\n",
      "11\tValidation loss: 307.704285\tBest loss: 0.218621\tAccuracy: 79.20%\n",
      "12\tValidation loss: 321.281982\tBest loss: 0.218621\tAccuracy: 72.95%\n",
      "13\tValidation loss: 228.562012\tBest loss: 0.218621\tAccuracy: 82.13%\n",
      "14\tValidation loss: 120.261292\tBest loss: 0.218621\tAccuracy: 90.23%\n",
      "15\tValidation loss: 411.611328\tBest loss: 0.218621\tAccuracy: 74.39%\n",
      "16\tValidation loss: 122.022614\tBest loss: 0.218621\tAccuracy: 89.41%\n",
      "17\tValidation loss: 160.043289\tBest loss: 0.218621\tAccuracy: 91.13%\n",
      "18\tValidation loss: 150.217377\tBest loss: 0.218621\tAccuracy: 91.44%\n",
      "19\tValidation loss: 218.242691\tBest loss: 0.218621\tAccuracy: 91.91%\n",
      "20\tValidation loss: 136.516251\tBest loss: 0.218621\tAccuracy: 93.20%\n",
      "21\tValidation loss: 153.304932\tBest loss: 0.218621\tAccuracy: 86.43%\n",
      "22\tValidation loss: 90.573509\tBest loss: 0.218621\tAccuracy: 93.98%\n",
      "23\tValidation loss: 124.441261\tBest loss: 0.218621\tAccuracy: 92.57%\n",
      "24\tValidation loss: 86.795662\tBest loss: 0.218621\tAccuracy: 93.08%\n",
      "25\tValidation loss: 129.022736\tBest loss: 0.218621\tAccuracy: 92.34%\n",
      "26\tValidation loss: 68.123360\tBest loss: 0.218621\tAccuracy: 92.03%\n",
      "27\tValidation loss: 65.781998\tBest loss: 0.218621\tAccuracy: 93.98%\n",
      "28\tValidation loss: 85.411903\tBest loss: 0.218621\tAccuracy: 89.95%\n",
      "29\tValidation loss: 4624.363770\tBest loss: 0.218621\tAccuracy: 71.74%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.05, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=50, n_neurons=90, total=  19.7s\n",
      "[CV] learning_rate=0.05, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=50, n_neurons=90 \n",
      "0\tValidation loss: 114.110184\tBest loss: 114.110184\tAccuracy: 22.67%\n",
      "1\tValidation loss: 3.077289\tBest loss: 3.077289\tAccuracy: 31.67%\n",
      "2\tValidation loss: 4591.471680\tBest loss: 3.077289\tAccuracy: 26.19%\n",
      "3\tValidation loss: 64.679543\tBest loss: 3.077289\tAccuracy: 68.33%\n",
      "4\tValidation loss: 69.305069\tBest loss: 3.077289\tAccuracy: 71.38%\n",
      "5\tValidation loss: 29.570536\tBest loss: 3.077289\tAccuracy: 83.97%\n",
      "6\tValidation loss: 42.804611\tBest loss: 3.077289\tAccuracy: 78.38%\n",
      "7\tValidation loss: 30.430555\tBest loss: 3.077289\tAccuracy: 85.50%\n",
      "8\tValidation loss: 22.948727\tBest loss: 3.077289\tAccuracy: 84.40%\n",
      "9\tValidation loss: 15.164020\tBest loss: 3.077289\tAccuracy: 91.36%\n",
      "10\tValidation loss: 15.100549\tBest loss: 3.077289\tAccuracy: 93.16%\n",
      "11\tValidation loss: 1710.827393\tBest loss: 3.077289\tAccuracy: 41.67%\n",
      "12\tValidation loss: 204.663437\tBest loss: 3.077289\tAccuracy: 71.34%\n",
      "13\tValidation loss: 127.160561\tBest loss: 3.077289\tAccuracy: 72.83%\n",
      "14\tValidation loss: 309.407349\tBest loss: 3.077289\tAccuracy: 73.30%\n",
      "15\tValidation loss: 79.319824\tBest loss: 3.077289\tAccuracy: 86.75%\n",
      "16\tValidation loss: 174.735703\tBest loss: 3.077289\tAccuracy: 75.61%\n",
      "17\tValidation loss: 88.649399\tBest loss: 3.077289\tAccuracy: 85.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\tValidation loss: 130.129227\tBest loss: 3.077289\tAccuracy: 86.32%\n",
      "19\tValidation loss: 113.409538\tBest loss: 3.077289\tAccuracy: 81.20%\n",
      "20\tValidation loss: 48.947777\tBest loss: 3.077289\tAccuracy: 87.37%\n",
      "21\tValidation loss: 65.315582\tBest loss: 3.077289\tAccuracy: 82.80%\n",
      "22\tValidation loss: 46.684887\tBest loss: 3.077289\tAccuracy: 92.14%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.05, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=50, n_neurons=90, total=  15.8s\n",
      "[CV] learning_rate=0.1, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123b9b400>, batch_size=50, n_neurons=70 \n",
      "0\tValidation loss: 1103.858032\tBest loss: 1103.858032\tAccuracy: 94.45%\n",
      "1\tValidation loss: 324.238739\tBest loss: 324.238739\tAccuracy: 95.23%\n",
      "2\tValidation loss: 143.448059\tBest loss: 143.448059\tAccuracy: 95.93%\n",
      "3\tValidation loss: 74.690666\tBest loss: 74.690666\tAccuracy: 95.70%\n",
      "4\tValidation loss: 89.043831\tBest loss: 74.690666\tAccuracy: 94.68%\n",
      "5\tValidation loss: 92.383263\tBest loss: 74.690666\tAccuracy: 95.43%\n",
      "6\tValidation loss: 29.088305\tBest loss: 29.088305\tAccuracy: 95.90%\n",
      "7\tValidation loss: 39.691174\tBest loss: 29.088305\tAccuracy: 96.44%\n",
      "8\tValidation loss: 34.165119\tBest loss: 29.088305\tAccuracy: 96.13%\n",
      "9\tValidation loss: 49.424549\tBest loss: 29.088305\tAccuracy: 94.18%\n",
      "10\tValidation loss: 41.419144\tBest loss: 29.088305\tAccuracy: 94.53%\n",
      "11\tValidation loss: 107915.273438\tBest loss: 29.088305\tAccuracy: 94.33%\n",
      "12\tValidation loss: 33189.996094\tBest loss: 29.088305\tAccuracy: 95.23%\n",
      "13\tValidation loss: 23743.972656\tBest loss: 29.088305\tAccuracy: 94.92%\n",
      "14\tValidation loss: 15076.444336\tBest loss: 29.088305\tAccuracy: 95.39%\n",
      "15\tValidation loss: 15171.568359\tBest loss: 29.088305\tAccuracy: 93.71%\n",
      "16\tValidation loss: 9499.283203\tBest loss: 29.088305\tAccuracy: 96.29%\n",
      "17\tValidation loss: 9382.034180\tBest loss: 29.088305\tAccuracy: 95.54%\n",
      "18\tValidation loss: 6703.875488\tBest loss: 29.088305\tAccuracy: 96.25%\n",
      "19\tValidation loss: 6655.593262\tBest loss: 29.088305\tAccuracy: 95.04%\n",
      "20\tValidation loss: 7207.263672\tBest loss: 29.088305\tAccuracy: 95.50%\n",
      "21\tValidation loss: 8867.680664\tBest loss: 29.088305\tAccuracy: 96.40%\n",
      "22\tValidation loss: 110107.843750\tBest loss: 29.088305\tAccuracy: 92.89%\n",
      "23\tValidation loss: 12753.654297\tBest loss: 29.088305\tAccuracy: 95.39%\n",
      "24\tValidation loss: 40061.820312\tBest loss: 29.088305\tAccuracy: 95.31%\n",
      "25\tValidation loss: 20259.667969\tBest loss: 29.088305\tAccuracy: 94.21%\n",
      "26\tValidation loss: 22456.457031\tBest loss: 29.088305\tAccuracy: 88.66%\n",
      "27\tValidation loss: 11344711.000000\tBest loss: 29.088305\tAccuracy: 76.39%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.1, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123b9b400>, batch_size=50, n_neurons=70, total=  17.7s\n",
      "[CV] learning_rate=0.1, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123b9b400>, batch_size=50, n_neurons=70 \n",
      "0\tValidation loss: 4305.335449\tBest loss: 4305.335449\tAccuracy: 91.67%\n",
      "1\tValidation loss: 1200.477661\tBest loss: 1200.477661\tAccuracy: 91.24%\n",
      "2\tValidation loss: 501.312164\tBest loss: 501.312164\tAccuracy: 92.65%\n",
      "3\tValidation loss: 567.509949\tBest loss: 501.312164\tAccuracy: 92.38%\n",
      "4\tValidation loss: 225.401779\tBest loss: 225.401779\tAccuracy: 95.31%\n",
      "5\tValidation loss: 291.006256\tBest loss: 225.401779\tAccuracy: 93.98%\n",
      "6\tValidation loss: 160.452301\tBest loss: 160.452301\tAccuracy: 94.92%\n",
      "7\tValidation loss: 126.595604\tBest loss: 126.595604\tAccuracy: 96.13%\n",
      "8\tValidation loss: 148.902512\tBest loss: 126.595604\tAccuracy: 96.48%\n",
      "9\tValidation loss: 192.585571\tBest loss: 126.595604\tAccuracy: 83.19%\n",
      "10\tValidation loss: 5035497.000000\tBest loss: 126.595604\tAccuracy: 25.41%\n",
      "11\tValidation loss: 21633.824219\tBest loss: 126.595604\tAccuracy: 90.11%\n",
      "12\tValidation loss: 10903.479492\tBest loss: 126.595604\tAccuracy: 91.83%\n",
      "13\tValidation loss: 10227.387695\tBest loss: 126.595604\tAccuracy: 91.36%\n",
      "14\tValidation loss: 9005.139648\tBest loss: 126.595604\tAccuracy: 93.43%\n",
      "15\tValidation loss: 5706.745117\tBest loss: 126.595604\tAccuracy: 94.96%\n",
      "16\tValidation loss: 9930.471680\tBest loss: 126.595604\tAccuracy: 93.59%\n",
      "17\tValidation loss: 9855.323242\tBest loss: 126.595604\tAccuracy: 93.75%\n",
      "18\tValidation loss: 10887.802734\tBest loss: 126.595604\tAccuracy: 90.42%\n",
      "19\tValidation loss: 12326.992188\tBest loss: 126.595604\tAccuracy: 90.34%\n",
      "20\tValidation loss: 5420.963379\tBest loss: 126.595604\tAccuracy: 96.01%\n",
      "21\tValidation loss: 4694.240234\tBest loss: 126.595604\tAccuracy: 96.05%\n",
      "22\tValidation loss: 7068.442383\tBest loss: 126.595604\tAccuracy: 97.11%\n",
      "23\tValidation loss: 6844.395508\tBest loss: 126.595604\tAccuracy: 93.63%\n",
      "24\tValidation loss: 8417.480469\tBest loss: 126.595604\tAccuracy: 93.67%\n",
      "25\tValidation loss: 2373.976562\tBest loss: 126.595604\tAccuracy: 96.87%\n",
      "26\tValidation loss: 130045.046875\tBest loss: 126.595604\tAccuracy: 94.76%\n",
      "27\tValidation loss: 36693.968750\tBest loss: 126.595604\tAccuracy: 95.82%\n",
      "28\tValidation loss: 22830.925781\tBest loss: 126.595604\tAccuracy: 95.23%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.1, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123b9b400>, batch_size=50, n_neurons=70, total=  18.5s\n",
      "[CV] learning_rate=0.1, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123b9b400>, batch_size=50, n_neurons=70 \n",
      "0\tValidation loss: 0.213686\tBest loss: 0.213686\tAccuracy: 94.41%\n",
      "1\tValidation loss: 0.224546\tBest loss: 0.213686\tAccuracy: 95.47%\n",
      "2\tValidation loss: 37563.410156\tBest loss: 0.213686\tAccuracy: 77.83%\n",
      "3\tValidation loss: 7069.728027\tBest loss: 0.213686\tAccuracy: 92.61%\n",
      "4\tValidation loss: 59816.460938\tBest loss: 0.213686\tAccuracy: 70.45%\n",
      "5\tValidation loss: 6851.232422\tBest loss: 0.213686\tAccuracy: 92.26%\n",
      "6\tValidation loss: 9454.356445\tBest loss: 0.213686\tAccuracy: 89.99%\n",
      "7\tValidation loss: 3366.489502\tBest loss: 0.213686\tAccuracy: 94.68%\n",
      "8\tValidation loss: 4317.312012\tBest loss: 0.213686\tAccuracy: 96.36%\n",
      "9\tValidation loss: 45976.109375\tBest loss: 0.213686\tAccuracy: 94.29%\n",
      "10\tValidation loss: 5467.927246\tBest loss: 0.213686\tAccuracy: 95.74%\n",
      "11\tValidation loss: 2243.677979\tBest loss: 0.213686\tAccuracy: 97.03%\n",
      "12\tValidation loss: 3621.292480\tBest loss: 0.213686\tAccuracy: 93.82%\n",
      "13\tValidation loss: 3191.678223\tBest loss: 0.213686\tAccuracy: 96.79%\n",
      "14\tValidation loss: 861050.187500\tBest loss: 0.213686\tAccuracy: 94.45%\n",
      "15\tValidation loss: 841943.437500\tBest loss: 0.213686\tAccuracy: 93.08%\n",
      "16\tValidation loss: 109830.226562\tBest loss: 0.213686\tAccuracy: 96.13%\n",
      "17\tValidation loss: 92929.140625\tBest loss: 0.213686\tAccuracy: 96.60%\n",
      "18\tValidation loss: 76270.007812\tBest loss: 0.213686\tAccuracy: 96.25%\n",
      "19\tValidation loss: 66925.179688\tBest loss: 0.213686\tAccuracy: 95.66%\n",
      "20\tValidation loss: 80923.398438\tBest loss: 0.213686\tAccuracy: 95.70%\n",
      "21\tValidation loss: 38930.167969\tBest loss: 0.213686\tAccuracy: 97.03%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.1, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123b9b400>, batch_size=50, n_neurons=70, total=  13.6s\n",
      "[CV] learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=120 \n",
      "0\tValidation loss: 0.097257\tBest loss: 0.097257\tAccuracy: 97.26%\n",
      "1\tValidation loss: 0.060334\tBest loss: 0.060334\tAccuracy: 98.20%\n",
      "2\tValidation loss: 0.078829\tBest loss: 0.060334\tAccuracy: 97.54%\n",
      "3\tValidation loss: 0.047344\tBest loss: 0.047344\tAccuracy: 98.55%\n",
      "4\tValidation loss: 0.063048\tBest loss: 0.047344\tAccuracy: 98.32%\n",
      "5\tValidation loss: 0.049553\tBest loss: 0.047344\tAccuracy: 98.59%\n",
      "6\tValidation loss: 0.058619\tBest loss: 0.047344\tAccuracy: 98.44%\n",
      "7\tValidation loss: 0.053147\tBest loss: 0.047344\tAccuracy: 98.83%\n",
      "8\tValidation loss: 0.049796\tBest loss: 0.047344\tAccuracy: 98.83%\n",
      "9\tValidation loss: 0.053676\tBest loss: 0.047344\tAccuracy: 98.67%\n",
      "10\tValidation loss: 0.053228\tBest loss: 0.047344\tAccuracy: 98.91%\n",
      "11\tValidation loss: 0.063255\tBest loss: 0.047344\tAccuracy: 98.51%\n",
      "12\tValidation loss: 0.063146\tBest loss: 0.047344\tAccuracy: 98.67%\n",
      "13\tValidation loss: 0.054567\tBest loss: 0.047344\tAccuracy: 98.75%\n",
      "14\tValidation loss: 0.070330\tBest loss: 0.047344\tAccuracy: 98.67%\n",
      "15\tValidation loss: 0.085590\tBest loss: 0.047344\tAccuracy: 98.59%\n",
      "16\tValidation loss: 0.048055\tBest loss: 0.047344\tAccuracy: 99.02%\n",
      "17\tValidation loss: 0.058492\tBest loss: 0.047344\tAccuracy: 98.87%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\tValidation loss: 0.078608\tBest loss: 0.047344\tAccuracy: 98.67%\n",
      "19\tValidation loss: 0.068237\tBest loss: 0.047344\tAccuracy: 98.79%\n",
      "20\tValidation loss: 0.073818\tBest loss: 0.047344\tAccuracy: 98.63%\n",
      "21\tValidation loss: 0.065038\tBest loss: 0.047344\tAccuracy: 98.75%\n",
      "22\tValidation loss: 0.063429\tBest loss: 0.047344\tAccuracy: 98.67%\n",
      "23\tValidation loss: 0.064122\tBest loss: 0.047344\tAccuracy: 98.63%\n",
      "24\tValidation loss: 0.068853\tBest loss: 0.047344\tAccuracy: 98.75%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=120, total=   8.0s\n",
      "[CV] learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=120 \n",
      "0\tValidation loss: 0.091282\tBest loss: 0.091282\tAccuracy: 96.91%\n",
      "1\tValidation loss: 0.055147\tBest loss: 0.055147\tAccuracy: 98.28%\n",
      "2\tValidation loss: 0.052058\tBest loss: 0.052058\tAccuracy: 98.40%\n",
      "3\tValidation loss: 0.045157\tBest loss: 0.045157\tAccuracy: 98.59%\n",
      "4\tValidation loss: 0.046581\tBest loss: 0.045157\tAccuracy: 98.63%\n",
      "5\tValidation loss: 0.059349\tBest loss: 0.045157\tAccuracy: 98.28%\n",
      "6\tValidation loss: 0.052693\tBest loss: 0.045157\tAccuracy: 98.63%\n",
      "7\tValidation loss: 0.045397\tBest loss: 0.045157\tAccuracy: 98.98%\n",
      "8\tValidation loss: 0.066202\tBest loss: 0.045157\tAccuracy: 98.20%\n",
      "9\tValidation loss: 0.052724\tBest loss: 0.045157\tAccuracy: 98.71%\n",
      "10\tValidation loss: 0.060415\tBest loss: 0.045157\tAccuracy: 98.75%\n",
      "11\tValidation loss: 0.052719\tBest loss: 0.045157\tAccuracy: 98.83%\n",
      "12\tValidation loss: 0.065357\tBest loss: 0.045157\tAccuracy: 98.28%\n",
      "13\tValidation loss: 0.049780\tBest loss: 0.045157\tAccuracy: 98.67%\n",
      "14\tValidation loss: 0.046924\tBest loss: 0.045157\tAccuracy: 98.75%\n",
      "15\tValidation loss: 0.059700\tBest loss: 0.045157\tAccuracy: 98.44%\n",
      "16\tValidation loss: 0.042521\tBest loss: 0.042521\tAccuracy: 98.91%\n",
      "17\tValidation loss: 0.079620\tBest loss: 0.042521\tAccuracy: 98.59%\n",
      "18\tValidation loss: 0.052363\tBest loss: 0.042521\tAccuracy: 98.94%\n",
      "19\tValidation loss: 0.053405\tBest loss: 0.042521\tAccuracy: 98.83%\n",
      "20\tValidation loss: 0.059019\tBest loss: 0.042521\tAccuracy: 98.91%\n",
      "21\tValidation loss: 0.060585\tBest loss: 0.042521\tAccuracy: 99.10%\n",
      "22\tValidation loss: 0.057765\tBest loss: 0.042521\tAccuracy: 98.87%\n",
      "23\tValidation loss: 0.057100\tBest loss: 0.042521\tAccuracy: 99.02%\n",
      "24\tValidation loss: 0.067737\tBest loss: 0.042521\tAccuracy: 99.10%\n",
      "25\tValidation loss: 0.051934\tBest loss: 0.042521\tAccuracy: 99.10%\n",
      "26\tValidation loss: 0.056081\tBest loss: 0.042521\tAccuracy: 98.91%\n",
      "27\tValidation loss: 0.054746\tBest loss: 0.042521\tAccuracy: 99.10%\n",
      "28\tValidation loss: 0.065851\tBest loss: 0.042521\tAccuracy: 98.94%\n",
      "29\tValidation loss: 0.053121\tBest loss: 0.042521\tAccuracy: 99.06%\n",
      "30\tValidation loss: 0.069190\tBest loss: 0.042521\tAccuracy: 98.36%\n",
      "31\tValidation loss: 0.062104\tBest loss: 0.042521\tAccuracy: 98.87%\n",
      "32\tValidation loss: 0.075229\tBest loss: 0.042521\tAccuracy: 98.75%\n",
      "33\tValidation loss: 0.074459\tBest loss: 0.042521\tAccuracy: 98.79%\n",
      "34\tValidation loss: 0.068969\tBest loss: 0.042521\tAccuracy: 98.71%\n",
      "35\tValidation loss: 0.082942\tBest loss: 0.042521\tAccuracy: 98.75%\n",
      "36\tValidation loss: 0.074678\tBest loss: 0.042521\tAccuracy: 98.59%\n",
      "37\tValidation loss: 0.083865\tBest loss: 0.042521\tAccuracy: 98.67%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=120, total=  10.8s\n",
      "[CV] learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=120 \n",
      "0\tValidation loss: 0.083784\tBest loss: 0.083784\tAccuracy: 97.38%\n",
      "1\tValidation loss: 0.062163\tBest loss: 0.062163\tAccuracy: 98.32%\n",
      "2\tValidation loss: 0.058925\tBest loss: 0.058925\tAccuracy: 98.05%\n",
      "3\tValidation loss: 0.058891\tBest loss: 0.058891\tAccuracy: 98.44%\n",
      "4\tValidation loss: 0.056716\tBest loss: 0.056716\tAccuracy: 98.32%\n",
      "5\tValidation loss: 0.075397\tBest loss: 0.056716\tAccuracy: 98.12%\n",
      "6\tValidation loss: 0.059455\tBest loss: 0.056716\tAccuracy: 98.40%\n",
      "7\tValidation loss: 0.044798\tBest loss: 0.044798\tAccuracy: 98.75%\n",
      "8\tValidation loss: 0.065449\tBest loss: 0.044798\tAccuracy: 98.40%\n",
      "9\tValidation loss: 0.055277\tBest loss: 0.044798\tAccuracy: 98.79%\n",
      "10\tValidation loss: 0.069357\tBest loss: 0.044798\tAccuracy: 98.71%\n",
      "11\tValidation loss: 0.066891\tBest loss: 0.044798\tAccuracy: 98.79%\n",
      "12\tValidation loss: 0.070156\tBest loss: 0.044798\tAccuracy: 98.79%\n",
      "13\tValidation loss: 0.063409\tBest loss: 0.044798\tAccuracy: 98.71%\n",
      "14\tValidation loss: 0.066600\tBest loss: 0.044798\tAccuracy: 98.67%\n",
      "15\tValidation loss: 0.060315\tBest loss: 0.044798\tAccuracy: 98.55%\n",
      "16\tValidation loss: 0.063839\tBest loss: 0.044798\tAccuracy: 98.75%\n",
      "17\tValidation loss: 0.047497\tBest loss: 0.044798\tAccuracy: 99.14%\n",
      "18\tValidation loss: 0.083096\tBest loss: 0.044798\tAccuracy: 98.59%\n",
      "19\tValidation loss: 0.061638\tBest loss: 0.044798\tAccuracy: 98.59%\n",
      "20\tValidation loss: 0.053634\tBest loss: 0.044798\tAccuracy: 98.87%\n",
      "21\tValidation loss: 0.085097\tBest loss: 0.044798\tAccuracy: 98.71%\n",
      "22\tValidation loss: 0.061718\tBest loss: 0.044798\tAccuracy: 99.10%\n",
      "23\tValidation loss: 0.085943\tBest loss: 0.044798\tAccuracy: 98.75%\n",
      "24\tValidation loss: 0.073285\tBest loss: 0.044798\tAccuracy: 98.44%\n",
      "25\tValidation loss: 0.076007\tBest loss: 0.044798\tAccuracy: 98.28%\n",
      "26\tValidation loss: 0.059669\tBest loss: 0.044798\tAccuracy: 98.83%\n",
      "27\tValidation loss: 0.058963\tBest loss: 0.044798\tAccuracy: 99.02%\n",
      "28\tValidation loss: 0.069180\tBest loss: 0.044798\tAccuracy: 98.87%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=120, total=   8.9s\n",
      "[CV] learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=90 \n",
      "0\tValidation loss: 0.096998\tBest loss: 0.096998\tAccuracy: 97.19%\n",
      "1\tValidation loss: 0.069542\tBest loss: 0.069542\tAccuracy: 97.89%\n",
      "2\tValidation loss: 0.065913\tBest loss: 0.065913\tAccuracy: 97.93%\n",
      "3\tValidation loss: 0.058136\tBest loss: 0.058136\tAccuracy: 98.32%\n",
      "4\tValidation loss: 0.061067\tBest loss: 0.058136\tAccuracy: 98.16%\n",
      "5\tValidation loss: 0.071278\tBest loss: 0.058136\tAccuracy: 98.01%\n",
      "6\tValidation loss: 0.053290\tBest loss: 0.053290\tAccuracy: 98.55%\n",
      "7\tValidation loss: 0.074718\tBest loss: 0.053290\tAccuracy: 97.97%\n",
      "8\tValidation loss: 0.058758\tBest loss: 0.053290\tAccuracy: 98.36%\n",
      "9\tValidation loss: 0.041782\tBest loss: 0.041782\tAccuracy: 98.91%\n",
      "10\tValidation loss: 0.077254\tBest loss: 0.041782\tAccuracy: 98.48%\n",
      "11\tValidation loss: 0.042601\tBest loss: 0.041782\tAccuracy: 98.91%\n",
      "12\tValidation loss: 0.060358\tBest loss: 0.041782\tAccuracy: 98.67%\n",
      "13\tValidation loss: 0.060191\tBest loss: 0.041782\tAccuracy: 98.67%\n",
      "14\tValidation loss: 0.070685\tBest loss: 0.041782\tAccuracy: 98.08%\n",
      "15\tValidation loss: 0.054294\tBest loss: 0.041782\tAccuracy: 98.91%\n",
      "16\tValidation loss: 0.055328\tBest loss: 0.041782\tAccuracy: 98.94%\n",
      "17\tValidation loss: 0.076926\tBest loss: 0.041782\tAccuracy: 98.40%\n",
      "18\tValidation loss: 0.056676\tBest loss: 0.041782\tAccuracy: 98.63%\n",
      "19\tValidation loss: 0.052445\tBest loss: 0.041782\tAccuracy: 98.91%\n",
      "20\tValidation loss: 0.068074\tBest loss: 0.041782\tAccuracy: 98.67%\n",
      "21\tValidation loss: 0.058952\tBest loss: 0.041782\tAccuracy: 98.87%\n",
      "22\tValidation loss: 0.063994\tBest loss: 0.041782\tAccuracy: 98.79%\n",
      "23\tValidation loss: 0.065980\tBest loss: 0.041782\tAccuracy: 98.75%\n",
      "24\tValidation loss: 0.054433\tBest loss: 0.041782\tAccuracy: 98.98%\n",
      "25\tValidation loss: 0.076099\tBest loss: 0.041782\tAccuracy: 98.51%\n",
      "26\tValidation loss: 0.075772\tBest loss: 0.041782\tAccuracy: 98.36%\n",
      "27\tValidation loss: 0.052322\tBest loss: 0.041782\tAccuracy: 99.14%\n",
      "28\tValidation loss: 0.061812\tBest loss: 0.041782\tAccuracy: 98.67%\n",
      "29\tValidation loss: 0.045295\tBest loss: 0.041782\tAccuracy: 98.98%\n",
      "30\tValidation loss: 0.053627\tBest loss: 0.041782\tAccuracy: 99.02%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=90, total=   7.7s\n",
      "[CV] learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=90 \n",
      "0\tValidation loss: 0.094712\tBest loss: 0.094712\tAccuracy: 97.11%\n",
      "1\tValidation loss: 0.063119\tBest loss: 0.063119\tAccuracy: 98.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\tValidation loss: 0.054619\tBest loss: 0.054619\tAccuracy: 98.36%\n",
      "3\tValidation loss: 0.058786\tBest loss: 0.054619\tAccuracy: 98.20%\n",
      "4\tValidation loss: 0.060169\tBest loss: 0.054619\tAccuracy: 97.93%\n",
      "5\tValidation loss: 0.051173\tBest loss: 0.051173\tAccuracy: 98.51%\n",
      "6\tValidation loss: 0.058635\tBest loss: 0.051173\tAccuracy: 98.51%\n",
      "7\tValidation loss: 0.048505\tBest loss: 0.048505\tAccuracy: 98.44%\n",
      "8\tValidation loss: 0.042076\tBest loss: 0.042076\tAccuracy: 98.87%\n",
      "9\tValidation loss: 0.054112\tBest loss: 0.042076\tAccuracy: 98.79%\n",
      "10\tValidation loss: 0.054472\tBest loss: 0.042076\tAccuracy: 98.63%\n",
      "11\tValidation loss: 0.049981\tBest loss: 0.042076\tAccuracy: 98.67%\n",
      "12\tValidation loss: 0.056388\tBest loss: 0.042076\tAccuracy: 98.59%\n",
      "13\tValidation loss: 0.067063\tBest loss: 0.042076\tAccuracy: 98.51%\n",
      "14\tValidation loss: 0.056505\tBest loss: 0.042076\tAccuracy: 98.55%\n",
      "15\tValidation loss: 0.077752\tBest loss: 0.042076\tAccuracy: 98.48%\n",
      "16\tValidation loss: 0.064870\tBest loss: 0.042076\tAccuracy: 98.55%\n",
      "17\tValidation loss: 0.053615\tBest loss: 0.042076\tAccuracy: 98.98%\n",
      "18\tValidation loss: 0.051140\tBest loss: 0.042076\tAccuracy: 98.98%\n",
      "19\tValidation loss: 0.061469\tBest loss: 0.042076\tAccuracy: 98.83%\n",
      "20\tValidation loss: 0.067056\tBest loss: 0.042076\tAccuracy: 98.94%\n",
      "21\tValidation loss: 0.069569\tBest loss: 0.042076\tAccuracy: 98.63%\n",
      "22\tValidation loss: 0.036243\tBest loss: 0.036243\tAccuracy: 99.06%\n",
      "23\tValidation loss: 0.049581\tBest loss: 0.036243\tAccuracy: 98.87%\n",
      "24\tValidation loss: 0.046064\tBest loss: 0.036243\tAccuracy: 98.87%\n",
      "25\tValidation loss: 0.069283\tBest loss: 0.036243\tAccuracy: 98.94%\n",
      "26\tValidation loss: 0.076329\tBest loss: 0.036243\tAccuracy: 98.79%\n",
      "27\tValidation loss: 0.057697\tBest loss: 0.036243\tAccuracy: 99.02%\n",
      "28\tValidation loss: 0.090615\tBest loss: 0.036243\tAccuracy: 98.67%\n",
      "29\tValidation loss: 0.049713\tBest loss: 0.036243\tAccuracy: 98.79%\n",
      "30\tValidation loss: 0.062234\tBest loss: 0.036243\tAccuracy: 99.02%\n",
      "31\tValidation loss: 0.059555\tBest loss: 0.036243\tAccuracy: 98.98%\n",
      "32\tValidation loss: 0.067023\tBest loss: 0.036243\tAccuracy: 98.59%\n",
      "33\tValidation loss: 0.070388\tBest loss: 0.036243\tAccuracy: 98.71%\n",
      "34\tValidation loss: 0.067641\tBest loss: 0.036243\tAccuracy: 98.71%\n",
      "35\tValidation loss: 0.054706\tBest loss: 0.036243\tAccuracy: 98.87%\n",
      "36\tValidation loss: 0.092976\tBest loss: 0.036243\tAccuracy: 98.75%\n",
      "37\tValidation loss: 0.072887\tBest loss: 0.036243\tAccuracy: 98.71%\n",
      "38\tValidation loss: 0.061033\tBest loss: 0.036243\tAccuracy: 98.67%\n",
      "39\tValidation loss: 0.058997\tBest loss: 0.036243\tAccuracy: 98.91%\n",
      "40\tValidation loss: 0.068990\tBest loss: 0.036243\tAccuracy: 98.71%\n",
      "41\tValidation loss: 0.059063\tBest loss: 0.036243\tAccuracy: 99.06%\n",
      "42\tValidation loss: 0.102423\tBest loss: 0.036243\tAccuracy: 98.63%\n",
      "43\tValidation loss: 0.083539\tBest loss: 0.036243\tAccuracy: 98.75%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=90, total=  10.6s\n",
      "[CV] learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=90 \n",
      "0\tValidation loss: 0.098127\tBest loss: 0.098127\tAccuracy: 97.34%\n",
      "1\tValidation loss: 0.070851\tBest loss: 0.070851\tAccuracy: 97.89%\n",
      "2\tValidation loss: 0.059604\tBest loss: 0.059604\tAccuracy: 98.24%\n",
      "3\tValidation loss: 0.056078\tBest loss: 0.056078\tAccuracy: 98.48%\n",
      "4\tValidation loss: 0.054594\tBest loss: 0.054594\tAccuracy: 98.67%\n",
      "5\tValidation loss: 0.059280\tBest loss: 0.054594\tAccuracy: 98.40%\n",
      "6\tValidation loss: 0.062750\tBest loss: 0.054594\tAccuracy: 98.12%\n",
      "7\tValidation loss: 0.077188\tBest loss: 0.054594\tAccuracy: 97.97%\n",
      "8\tValidation loss: 0.053669\tBest loss: 0.053669\tAccuracy: 98.79%\n",
      "9\tValidation loss: 0.060564\tBest loss: 0.053669\tAccuracy: 98.55%\n",
      "10\tValidation loss: 0.059070\tBest loss: 0.053669\tAccuracy: 98.51%\n",
      "11\tValidation loss: 0.058052\tBest loss: 0.053669\tAccuracy: 98.71%\n",
      "12\tValidation loss: 0.062364\tBest loss: 0.053669\tAccuracy: 98.48%\n",
      "13\tValidation loss: 0.058543\tBest loss: 0.053669\tAccuracy: 98.48%\n",
      "14\tValidation loss: 0.065018\tBest loss: 0.053669\tAccuracy: 98.83%\n",
      "15\tValidation loss: 0.067564\tBest loss: 0.053669\tAccuracy: 98.91%\n",
      "16\tValidation loss: 0.068966\tBest loss: 0.053669\tAccuracy: 98.28%\n",
      "17\tValidation loss: 0.051097\tBest loss: 0.051097\tAccuracy: 98.71%\n",
      "18\tValidation loss: 0.065292\tBest loss: 0.051097\tAccuracy: 98.75%\n",
      "19\tValidation loss: 0.048567\tBest loss: 0.048567\tAccuracy: 98.67%\n",
      "20\tValidation loss: 0.077399\tBest loss: 0.048567\tAccuracy: 98.63%\n",
      "21\tValidation loss: 0.063797\tBest loss: 0.048567\tAccuracy: 98.71%\n",
      "22\tValidation loss: 0.058065\tBest loss: 0.048567\tAccuracy: 98.83%\n",
      "23\tValidation loss: 0.066517\tBest loss: 0.048567\tAccuracy: 98.63%\n",
      "24\tValidation loss: 0.049760\tBest loss: 0.048567\tAccuracy: 98.98%\n",
      "25\tValidation loss: 0.081083\tBest loss: 0.048567\tAccuracy: 98.59%\n",
      "26\tValidation loss: 0.066356\tBest loss: 0.048567\tAccuracy: 98.98%\n",
      "27\tValidation loss: 0.094189\tBest loss: 0.048567\tAccuracy: 98.51%\n",
      "28\tValidation loss: 0.065410\tBest loss: 0.048567\tAccuracy: 98.75%\n",
      "29\tValidation loss: 0.060517\tBest loss: 0.048567\tAccuracy: 98.83%\n",
      "30\tValidation loss: 0.075410\tBest loss: 0.048567\tAccuracy: 98.67%\n",
      "31\tValidation loss: 0.051912\tBest loss: 0.048567\tAccuracy: 98.91%\n",
      "32\tValidation loss: 0.065697\tBest loss: 0.048567\tAccuracy: 98.67%\n",
      "33\tValidation loss: 0.051767\tBest loss: 0.048567\tAccuracy: 98.83%\n",
      "34\tValidation loss: 0.053112\tBest loss: 0.048567\tAccuracy: 98.67%\n",
      "35\tValidation loss: 0.067157\tBest loss: 0.048567\tAccuracy: 98.63%\n",
      "36\tValidation loss: 0.063080\tBest loss: 0.048567\tAccuracy: 98.91%\n",
      "37\tValidation loss: 0.056301\tBest loss: 0.048567\tAccuracy: 98.91%\n",
      "38\tValidation loss: 0.074941\tBest loss: 0.048567\tAccuracy: 98.87%\n",
      "39\tValidation loss: 0.059339\tBest loss: 0.048567\tAccuracy: 98.83%\n",
      "40\tValidation loss: 0.062603\tBest loss: 0.048567\tAccuracy: 98.87%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.01, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x123d71d08>, batch_size=500, n_neurons=90, total=  10.1s\n",
      "[CV] learning_rate=0.01, activation=<function elu at 0x11b8a9d08>, batch_size=500, n_neurons=140 \n",
      "0\tValidation loss: 0.124219\tBest loss: 0.124219\tAccuracy: 96.09%\n",
      "1\tValidation loss: 0.094653\tBest loss: 0.094653\tAccuracy: 97.03%\n",
      "2\tValidation loss: 0.081574\tBest loss: 0.081574\tAccuracy: 97.50%\n",
      "3\tValidation loss: 0.061818\tBest loss: 0.061818\tAccuracy: 98.01%\n",
      "4\tValidation loss: 0.071963\tBest loss: 0.061818\tAccuracy: 97.81%\n",
      "5\tValidation loss: 0.061996\tBest loss: 0.061818\tAccuracy: 98.28%\n",
      "6\tValidation loss: 0.054792\tBest loss: 0.054792\tAccuracy: 98.59%\n",
      "7\tValidation loss: 0.064103\tBest loss: 0.054792\tAccuracy: 98.32%\n",
      "8\tValidation loss: 0.062308\tBest loss: 0.054792\tAccuracy: 98.32%\n",
      "9\tValidation loss: 0.067285\tBest loss: 0.054792\tAccuracy: 98.51%\n",
      "10\tValidation loss: 0.060857\tBest loss: 0.054792\tAccuracy: 98.63%\n",
      "11\tValidation loss: 0.062337\tBest loss: 0.054792\tAccuracy: 98.63%\n",
      "12\tValidation loss: 0.068451\tBest loss: 0.054792\tAccuracy: 98.55%\n",
      "13\tValidation loss: 0.086291\tBest loss: 0.054792\tAccuracy: 98.32%\n",
      "14\tValidation loss: 0.085011\tBest loss: 0.054792\tAccuracy: 98.59%\n",
      "15\tValidation loss: 0.065911\tBest loss: 0.054792\tAccuracy: 98.67%\n",
      "16\tValidation loss: 0.066776\tBest loss: 0.054792\tAccuracy: 98.75%\n",
      "17\tValidation loss: 0.073785\tBest loss: 0.054792\tAccuracy: 98.79%\n",
      "18\tValidation loss: 0.065189\tBest loss: 0.054792\tAccuracy: 98.63%\n",
      "19\tValidation loss: 0.066529\tBest loss: 0.054792\tAccuracy: 98.51%\n",
      "20\tValidation loss: 0.062331\tBest loss: 0.054792\tAccuracy: 98.79%\n",
      "21\tValidation loss: 0.074050\tBest loss: 0.054792\tAccuracy: 98.75%\n",
      "22\tValidation loss: 0.073094\tBest loss: 0.054792\tAccuracy: 98.67%\n",
      "23\tValidation loss: 0.067294\tBest loss: 0.054792\tAccuracy: 98.63%\n",
      "24\tValidation loss: 0.065659\tBest loss: 0.054792\tAccuracy: 99.02%\n",
      "25\tValidation loss: 0.066115\tBest loss: 0.054792\tAccuracy: 98.83%\n",
      "26\tValidation loss: 0.066593\tBest loss: 0.054792\tAccuracy: 98.79%\n",
      "27\tValidation loss: 0.074258\tBest loss: 0.054792\tAccuracy: 99.06%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.01, activation=<function elu at 0x11b8a9d08>, batch_size=500, n_neurons=140, total=   8.6s\n",
      "[CV] learning_rate=0.01, activation=<function elu at 0x11b8a9d08>, batch_size=500, n_neurons=140 \n",
      "0\tValidation loss: 0.141076\tBest loss: 0.141076\tAccuracy: 95.11%\n",
      "1\tValidation loss: 0.093235\tBest loss: 0.093235\tAccuracy: 97.07%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\tValidation loss: 0.082130\tBest loss: 0.082130\tAccuracy: 97.54%\n",
      "3\tValidation loss: 0.071495\tBest loss: 0.071495\tAccuracy: 97.77%\n",
      "4\tValidation loss: 0.065252\tBest loss: 0.065252\tAccuracy: 98.12%\n",
      "5\tValidation loss: 0.063368\tBest loss: 0.063368\tAccuracy: 98.32%\n",
      "6\tValidation loss: 0.056379\tBest loss: 0.056379\tAccuracy: 98.36%\n",
      "7\tValidation loss: 0.046885\tBest loss: 0.046885\tAccuracy: 98.48%\n",
      "8\tValidation loss: 0.049021\tBest loss: 0.046885\tAccuracy: 98.55%\n",
      "9\tValidation loss: 0.046623\tBest loss: 0.046623\tAccuracy: 98.63%\n",
      "10\tValidation loss: 0.051268\tBest loss: 0.046623\tAccuracy: 98.55%\n",
      "11\tValidation loss: 0.048726\tBest loss: 0.046623\tAccuracy: 98.32%\n",
      "12\tValidation loss: 0.049078\tBest loss: 0.046623\tAccuracy: 98.67%\n",
      "13\tValidation loss: 0.064530\tBest loss: 0.046623\tAccuracy: 98.51%\n",
      "14\tValidation loss: 0.067596\tBest loss: 0.046623\tAccuracy: 98.51%\n",
      "15\tValidation loss: 0.050673\tBest loss: 0.046623\tAccuracy: 98.67%\n",
      "16\tValidation loss: 0.049015\tBest loss: 0.046623\tAccuracy: 98.67%\n",
      "17\tValidation loss: 0.053361\tBest loss: 0.046623\tAccuracy: 98.67%\n",
      "18\tValidation loss: 0.059181\tBest loss: 0.046623\tAccuracy: 98.75%\n",
      "19\tValidation loss: 0.063185\tBest loss: 0.046623\tAccuracy: 98.55%\n",
      "20\tValidation loss: 0.060685\tBest loss: 0.046623\tAccuracy: 98.59%\n",
      "21\tValidation loss: 0.101444\tBest loss: 0.046623\tAccuracy: 97.97%\n",
      "22\tValidation loss: 0.055676\tBest loss: 0.046623\tAccuracy: 99.02%\n",
      "23\tValidation loss: 0.073408\tBest loss: 0.046623\tAccuracy: 98.44%\n",
      "24\tValidation loss: 0.054970\tBest loss: 0.046623\tAccuracy: 98.87%\n",
      "25\tValidation loss: 0.059783\tBest loss: 0.046623\tAccuracy: 98.87%\n",
      "26\tValidation loss: 0.063396\tBest loss: 0.046623\tAccuracy: 98.83%\n",
      "27\tValidation loss: 0.058969\tBest loss: 0.046623\tAccuracy: 99.02%\n",
      "28\tValidation loss: 0.063183\tBest loss: 0.046623\tAccuracy: 98.94%\n",
      "29\tValidation loss: 0.064362\tBest loss: 0.046623\tAccuracy: 98.94%\n",
      "30\tValidation loss: 0.065223\tBest loss: 0.046623\tAccuracy: 98.98%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.01, activation=<function elu at 0x11b8a9d08>, batch_size=500, n_neurons=140, total=   9.5s\n",
      "[CV] learning_rate=0.01, activation=<function elu at 0x11b8a9d08>, batch_size=500, n_neurons=140 \n",
      "0\tValidation loss: 0.139010\tBest loss: 0.139010\tAccuracy: 95.47%\n",
      "1\tValidation loss: 0.081789\tBest loss: 0.081789\tAccuracy: 97.65%\n",
      "2\tValidation loss: 0.065703\tBest loss: 0.065703\tAccuracy: 97.93%\n",
      "3\tValidation loss: 0.054952\tBest loss: 0.054952\tAccuracy: 98.40%\n",
      "4\tValidation loss: 0.056066\tBest loss: 0.054952\tAccuracy: 98.28%\n",
      "5\tValidation loss: 0.052715\tBest loss: 0.052715\tAccuracy: 98.44%\n",
      "6\tValidation loss: 0.053864\tBest loss: 0.052715\tAccuracy: 98.40%\n",
      "7\tValidation loss: 0.049631\tBest loss: 0.049631\tAccuracy: 98.51%\n",
      "8\tValidation loss: 0.046010\tBest loss: 0.046010\tAccuracy: 98.67%\n",
      "9\tValidation loss: 0.047022\tBest loss: 0.046010\tAccuracy: 98.91%\n",
      "10\tValidation loss: 0.068965\tBest loss: 0.046010\tAccuracy: 98.32%\n",
      "11\tValidation loss: 0.054185\tBest loss: 0.046010\tAccuracy: 98.44%\n",
      "12\tValidation loss: 0.061987\tBest loss: 0.046010\tAccuracy: 98.59%\n",
      "13\tValidation loss: 0.054732\tBest loss: 0.046010\tAccuracy: 98.67%\n",
      "14\tValidation loss: 0.068161\tBest loss: 0.046010\tAccuracy: 98.36%\n",
      "15\tValidation loss: 0.071118\tBest loss: 0.046010\tAccuracy: 98.55%\n",
      "16\tValidation loss: 0.058385\tBest loss: 0.046010\tAccuracy: 98.44%\n",
      "17\tValidation loss: 0.043809\tBest loss: 0.043809\tAccuracy: 98.83%\n",
      "18\tValidation loss: 0.045579\tBest loss: 0.043809\tAccuracy: 98.79%\n",
      "19\tValidation loss: 0.074146\tBest loss: 0.043809\tAccuracy: 98.48%\n",
      "20\tValidation loss: 0.039294\tBest loss: 0.039294\tAccuracy: 99.06%\n",
      "21\tValidation loss: 0.045829\tBest loss: 0.039294\tAccuracy: 98.94%\n",
      "22\tValidation loss: 0.047933\tBest loss: 0.039294\tAccuracy: 99.10%\n",
      "23\tValidation loss: 0.043325\tBest loss: 0.039294\tAccuracy: 99.37%\n",
      "24\tValidation loss: 0.043188\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "25\tValidation loss: 0.043820\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "26\tValidation loss: 0.044215\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "27\tValidation loss: 0.044560\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "28\tValidation loss: 0.044953\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "29\tValidation loss: 0.045262\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "30\tValidation loss: 0.045715\tBest loss: 0.039294\tAccuracy: 99.37%\n",
      "31\tValidation loss: 0.045995\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "32\tValidation loss: 0.046202\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "33\tValidation loss: 0.046452\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "34\tValidation loss: 0.046797\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "35\tValidation loss: 0.047059\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "36\tValidation loss: 0.047205\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "37\tValidation loss: 0.047566\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "38\tValidation loss: 0.047825\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "39\tValidation loss: 0.048040\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "40\tValidation loss: 0.048177\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "41\tValidation loss: 0.048466\tBest loss: 0.039294\tAccuracy: 99.34%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.01, activation=<function elu at 0x11b8a9d08>, batch_size=500, n_neurons=140, total=  12.5s\n",
      "[CV] learning_rate=0.1, activation=<function relu at 0x11b8d3ae8>, batch_size=10, n_neurons=50 \n",
      "0\tValidation loss: 1.637794\tBest loss: 1.637794\tAccuracy: 19.27%\n",
      "1\tValidation loss: 1.621887\tBest loss: 1.621887\tAccuracy: 19.27%\n",
      "2\tValidation loss: 1.619832\tBest loss: 1.619832\tAccuracy: 19.27%\n",
      "3\tValidation loss: 1.616521\tBest loss: 1.616521\tAccuracy: 19.27%\n",
      "4\tValidation loss: 1.654961\tBest loss: 1.616521\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.621475\tBest loss: 1.616521\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.632695\tBest loss: 1.616521\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.614866\tBest loss: 1.614866\tAccuracy: 19.27%\n",
      "8\tValidation loss: 1.642610\tBest loss: 1.614866\tAccuracy: 19.27%\n",
      "9\tValidation loss: 1.639823\tBest loss: 1.614866\tAccuracy: 19.27%\n",
      "10\tValidation loss: 1.615378\tBest loss: 1.614866\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.630569\tBest loss: 1.614866\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.636738\tBest loss: 1.614866\tAccuracy: 19.08%\n",
      "13\tValidation loss: 1.613106\tBest loss: 1.613106\tAccuracy: 19.27%\n",
      "14\tValidation loss: 1.629267\tBest loss: 1.613106\tAccuracy: 19.08%\n",
      "15\tValidation loss: 1.617142\tBest loss: 1.613106\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.626923\tBest loss: 1.613106\tAccuracy: 18.73%\n",
      "17\tValidation loss: 1.608595\tBest loss: 1.608595\tAccuracy: 22.01%\n",
      "18\tValidation loss: 1.624731\tBest loss: 1.608595\tAccuracy: 19.27%\n",
      "19\tValidation loss: 1.624030\tBest loss: 1.608595\tAccuracy: 20.91%\n",
      "20\tValidation loss: 1.615449\tBest loss: 1.608595\tAccuracy: 19.27%\n",
      "21\tValidation loss: 1.629645\tBest loss: 1.608595\tAccuracy: 19.27%\n",
      "22\tValidation loss: 1.619090\tBest loss: 1.608595\tAccuracy: 19.27%\n",
      "23\tValidation loss: 1.621075\tBest loss: 1.608595\tAccuracy: 22.01%\n",
      "24\tValidation loss: 1.633985\tBest loss: 1.608595\tAccuracy: 22.01%\n",
      "25\tValidation loss: 1.647179\tBest loss: 1.608595\tAccuracy: 22.01%\n",
      "26\tValidation loss: 1.614125\tBest loss: 1.608595\tAccuracy: 19.08%\n",
      "27\tValidation loss: 1.638547\tBest loss: 1.608595\tAccuracy: 19.27%\n",
      "28\tValidation loss: 1.656266\tBest loss: 1.608595\tAccuracy: 22.01%\n",
      "29\tValidation loss: 1.617742\tBest loss: 1.608595\tAccuracy: 18.73%\n",
      "30\tValidation loss: 1.641872\tBest loss: 1.608595\tAccuracy: 19.27%\n",
      "31\tValidation loss: 1.631484\tBest loss: 1.608595\tAccuracy: 18.73%\n",
      "32\tValidation loss: 1.641180\tBest loss: 1.608595\tAccuracy: 18.73%\n",
      "33\tValidation loss: 1.647449\tBest loss: 1.608595\tAccuracy: 19.08%\n",
      "34\tValidation loss: 1.610285\tBest loss: 1.608595\tAccuracy: 22.01%\n",
      "35\tValidation loss: 1.624693\tBest loss: 1.608595\tAccuracy: 19.08%\n",
      "36\tValidation loss: 1.630855\tBest loss: 1.608595\tAccuracy: 18.73%\n",
      "37\tValidation loss: 1.632279\tBest loss: 1.608595\tAccuracy: 18.73%\n",
      "38\tValidation loss: 1.631024\tBest loss: 1.608595\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.1, activation=<function relu at 0x11b8d3ae8>, batch_size=10, n_neurons=50, total=  59.9s\n",
      "[CV] learning_rate=0.1, activation=<function relu at 0x11b8d3ae8>, batch_size=10, n_neurons=50 \n",
      "0\tValidation loss: 1.631882\tBest loss: 1.631882\tAccuracy: 22.01%\n",
      "1\tValidation loss: 1.644826\tBest loss: 1.631882\tAccuracy: 19.08%\n",
      "2\tValidation loss: 1.611659\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "3\tValidation loss: 1.614181\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "4\tValidation loss: 1.617815\tBest loss: 1.611659\tAccuracy: 22.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\tValidation loss: 1.624095\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.635101\tBest loss: 1.611659\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.630914\tBest loss: 1.611659\tAccuracy: 18.73%\n",
      "8\tValidation loss: 1.639863\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.611894\tBest loss: 1.611659\tAccuracy: 20.91%\n",
      "10\tValidation loss: 1.613292\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.614465\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.615519\tBest loss: 1.611659\tAccuracy: 20.91%\n",
      "13\tValidation loss: 1.609593\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "14\tValidation loss: 1.628134\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "15\tValidation loss: 1.613723\tBest loss: 1.609593\tAccuracy: 19.08%\n",
      "16\tValidation loss: 1.621646\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "17\tValidation loss: 1.612428\tBest loss: 1.609593\tAccuracy: 19.08%\n",
      "18\tValidation loss: 1.622170\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.611825\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "20\tValidation loss: 1.612158\tBest loss: 1.609593\tAccuracy: 20.91%\n",
      "21\tValidation loss: 1.657981\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.643383\tBest loss: 1.609593\tAccuracy: 19.27%\n",
      "23\tValidation loss: 1.647554\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "24\tValidation loss: 1.652203\tBest loss: 1.609593\tAccuracy: 18.73%\n",
      "25\tValidation loss: 1.610976\tBest loss: 1.609593\tAccuracy: 20.91%\n",
      "26\tValidation loss: 1.614098\tBest loss: 1.609593\tAccuracy: 20.91%\n",
      "27\tValidation loss: 1.614371\tBest loss: 1.609593\tAccuracy: 20.91%\n",
      "28\tValidation loss: 1.642984\tBest loss: 1.609593\tAccuracy: 19.08%\n",
      "29\tValidation loss: 1.614041\tBest loss: 1.609593\tAccuracy: 19.08%\n",
      "30\tValidation loss: 1.609285\tBest loss: 1.609285\tAccuracy: 22.01%\n",
      "31\tValidation loss: 1.634894\tBest loss: 1.609285\tAccuracy: 18.73%\n",
      "32\tValidation loss: 1.614006\tBest loss: 1.609285\tAccuracy: 19.08%\n",
      "33\tValidation loss: 1.619314\tBest loss: 1.609285\tAccuracy: 19.08%\n",
      "34\tValidation loss: 1.611677\tBest loss: 1.609285\tAccuracy: 22.01%\n",
      "35\tValidation loss: 1.638075\tBest loss: 1.609285\tAccuracy: 20.91%\n",
      "36\tValidation loss: 1.621402\tBest loss: 1.609285\tAccuracy: 22.01%\n",
      "37\tValidation loss: 1.617570\tBest loss: 1.609285\tAccuracy: 22.01%\n",
      "38\tValidation loss: 1.620770\tBest loss: 1.609285\tAccuracy: 20.91%\n",
      "39\tValidation loss: 1.627340\tBest loss: 1.609285\tAccuracy: 19.27%\n",
      "40\tValidation loss: 1.607887\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "41\tValidation loss: 1.634036\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "42\tValidation loss: 1.618335\tBest loss: 1.607887\tAccuracy: 20.91%\n",
      "43\tValidation loss: 1.618744\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "44\tValidation loss: 1.621666\tBest loss: 1.607887\tAccuracy: 19.08%\n",
      "45\tValidation loss: 1.629387\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "46\tValidation loss: 1.637660\tBest loss: 1.607887\tAccuracy: 18.73%\n",
      "47\tValidation loss: 1.609609\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "48\tValidation loss: 1.612161\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "49\tValidation loss: 1.643130\tBest loss: 1.607887\tAccuracy: 19.27%\n",
      "50\tValidation loss: 1.611989\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "51\tValidation loss: 1.619973\tBest loss: 1.607887\tAccuracy: 19.08%\n",
      "52\tValidation loss: 1.627019\tBest loss: 1.607887\tAccuracy: 19.08%\n",
      "53\tValidation loss: 1.618522\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "54\tValidation loss: 1.654810\tBest loss: 1.607887\tAccuracy: 18.73%\n",
      "55\tValidation loss: 1.645115\tBest loss: 1.607887\tAccuracy: 18.73%\n",
      "56\tValidation loss: 1.669473\tBest loss: 1.607887\tAccuracy: 19.08%\n",
      "57\tValidation loss: 1.619665\tBest loss: 1.607887\tAccuracy: 18.73%\n",
      "58\tValidation loss: 1.612547\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "59\tValidation loss: 1.619512\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "60\tValidation loss: 1.635963\tBest loss: 1.607887\tAccuracy: 18.73%\n",
      "61\tValidation loss: 1.627398\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  learning_rate=0.1, activation=<function relu at 0x11b8d3ae8>, batch_size=10, n_neurons=50, total= 1.6min\n",
      "[CV] learning_rate=0.1, activation=<function relu at 0x11b8d3ae8>, batch_size=10, n_neurons=50 \n",
      "0\tValidation loss: 1.880330\tBest loss: 1.880330\tAccuracy: 19.27%\n",
      "1\tValidation loss: 1.860581\tBest loss: 1.860581\tAccuracy: 19.27%\n",
      "2\tValidation loss: 1.850155\tBest loss: 1.850155\tAccuracy: 19.27%\n",
      "3\tValidation loss: 1.853039\tBest loss: 1.850155\tAccuracy: 19.27%\n",
      "4\tValidation loss: 1.850571\tBest loss: 1.850155\tAccuracy: 22.01%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-441a6d823b1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mrnd_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'X_valid'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_valid1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_valid'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_valid1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mrnd_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-131-f896b3e7ee94>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_epochs, X_valid, y_valid)\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mextra_update_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_update_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m   def make_callable(self,\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mbuild_results\u001b[0;34m(self, session, tensor_values)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mbuild_results\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m    296\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_contraction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def leaky_relu(alpha = 0.01):\n",
    "    def parametrized_leaky_relu(z, name = None):\n",
    "        return tf.maximum(alpha * z, z, name = name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "param_distribs = {\n",
    "    'n_neurons': [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    'batch_size': [10, 50, 100, 500],\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "    'activation': [tf.nn.relu, tf.nn.elu, leaky_relu(alpha = 0.01), leaky_relu(alpha = 0.1)],\n",
    "    # 'optimizer_class': [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum = 0.95)],\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(DNNClassifier(random_state = 42), param_distribs, n_iter = 50, fit_params = {'X_valid': X_valid1, 'y_valid': y_valid1, 'n_epochs': 1000}, random_state = 42, verbose = 2)\n",
    "\n",
    "rnd_search.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.072597\tBest loss: 0.072597\tAccuracy: 97.77%\n",
      "1\tValidation loss: 0.050215\tBest loss: 0.050215\tAccuracy: 98.40%\n",
      "2\tValidation loss: 0.054389\tBest loss: 0.050215\tAccuracy: 98.55%\n",
      "3\tValidation loss: 0.053495\tBest loss: 0.050215\tAccuracy: 98.40%\n",
      "4\tValidation loss: 0.044562\tBest loss: 0.044562\tAccuracy: 98.87%\n",
      "5\tValidation loss: 0.045834\tBest loss: 0.044562\tAccuracy: 98.75%\n",
      "6\tValidation loss: 0.043812\tBest loss: 0.043812\tAccuracy: 98.75%\n",
      "7\tValidation loss: 0.037045\tBest loss: 0.037045\tAccuracy: 99.02%\n",
      "8\tValidation loss: 0.055309\tBest loss: 0.037045\tAccuracy: 98.40%\n",
      "9\tValidation loss: 0.042635\tBest loss: 0.037045\tAccuracy: 98.67%\n",
      "10\tValidation loss: 0.054810\tBest loss: 0.037045\tAccuracy: 98.51%\n",
      "11\tValidation loss: 0.054007\tBest loss: 0.037045\tAccuracy: 98.67%\n",
      "12\tValidation loss: 0.074453\tBest loss: 0.037045\tAccuracy: 98.59%\n",
      "13\tValidation loss: 0.045931\tBest loss: 0.037045\tAccuracy: 98.71%\n",
      "14\tValidation loss: 0.056376\tBest loss: 0.037045\tAccuracy: 98.71%\n",
      "15\tValidation loss: 0.052903\tBest loss: 0.037045\tAccuracy: 99.06%\n",
      "16\tValidation loss: 0.047966\tBest loss: 0.037045\tAccuracy: 98.91%\n",
      "17\tValidation loss: 0.064810\tBest loss: 0.037045\tAccuracy: 98.83%\n",
      "18\tValidation loss: 0.040772\tBest loss: 0.037045\tAccuracy: 99.02%\n",
      "19\tValidation loss: 0.066293\tBest loss: 0.037045\tAccuracy: 98.75%\n",
      "20\tValidation loss: 0.062136\tBest loss: 0.037045\tAccuracy: 98.75%\n",
      "21\tValidation loss: 0.045899\tBest loss: 0.037045\tAccuracy: 98.87%\n",
      "22\tValidation loss: 0.064351\tBest loss: 0.037045\tAccuracy: 98.63%\n",
      "23\tValidation loss: 0.059689\tBest loss: 0.037045\tAccuracy: 99.06%\n",
      "24\tValidation loss: 0.060334\tBest loss: 0.037045\tAccuracy: 98.98%\n",
      "25\tValidation loss: 0.083387\tBest loss: 0.037045\tAccuracy: 98.48%\n",
      "26\tValidation loss: 0.051445\tBest loss: 0.037045\tAccuracy: 98.87%\n",
      "27\tValidation loss: 0.071729\tBest loss: 0.037045\tAccuracy: 98.75%\n",
      "28\tValidation loss: 0.067784\tBest loss: 0.037045\tAccuracy: 98.75%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125126c80>,\n",
       "       batch_norm_momentum=None, batch_size=500, dropout_rate=None,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x122b289d8>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=140,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                        n_neurons=140, random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99046507102549131"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf.save(\"./my_best_mnist_model_0_to_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.056868\tBest loss: 0.056868\tAccuracy: 98.16%\n",
      "1\tValidation loss: 0.043904\tBest loss: 0.043904\tAccuracy: 98.44%\n",
      "2\tValidation loss: 0.039608\tBest loss: 0.039608\tAccuracy: 98.83%\n",
      "3\tValidation loss: 0.044007\tBest loss: 0.039608\tAccuracy: 98.59%\n",
      "4\tValidation loss: 0.039557\tBest loss: 0.039557\tAccuracy: 98.87%\n",
      "5\tValidation loss: 0.043557\tBest loss: 0.039557\tAccuracy: 98.87%\n",
      "6\tValidation loss: 0.043551\tBest loss: 0.039557\tAccuracy: 98.71%\n",
      "7\tValidation loss: 0.037263\tBest loss: 0.037263\tAccuracy: 99.02%\n",
      "8\tValidation loss: 0.039309\tBest loss: 0.037263\tAccuracy: 98.91%\n",
      "9\tValidation loss: 0.033105\tBest loss: 0.033105\tAccuracy: 98.98%\n",
      "10\tValidation loss: 0.037057\tBest loss: 0.033105\tAccuracy: 99.02%\n",
      "11\tValidation loss: 0.050779\tBest loss: 0.033105\tAccuracy: 98.75%\n",
      "12\tValidation loss: 0.038030\tBest loss: 0.033105\tAccuracy: 98.91%\n",
      "13\tValidation loss: 0.040565\tBest loss: 0.033105\tAccuracy: 98.94%\n",
      "14\tValidation loss: 0.034815\tBest loss: 0.033105\tAccuracy: 99.14%\n",
      "15\tValidation loss: 0.028580\tBest loss: 0.028580\tAccuracy: 99.30%\n",
      "16\tValidation loss: 0.034004\tBest loss: 0.028580\tAccuracy: 99.26%\n",
      "17\tValidation loss: 0.037323\tBest loss: 0.028580\tAccuracy: 99.10%\n",
      "18\tValidation loss: 0.049557\tBest loss: 0.028580\tAccuracy: 98.44%\n",
      "19\tValidation loss: 0.053529\tBest loss: 0.028580\tAccuracy: 98.75%\n",
      "20\tValidation loss: 0.039955\tBest loss: 0.028580\tAccuracy: 99.06%\n",
      "21\tValidation loss: 0.050256\tBest loss: 0.028580\tAccuracy: 98.91%\n",
      "22\tValidation loss: 0.047697\tBest loss: 0.028580\tAccuracy: 98.98%\n",
      "23\tValidation loss: 0.051529\tBest loss: 0.028580\tAccuracy: 98.98%\n",
      "24\tValidation loss: 0.042908\tBest loss: 0.028580\tAccuracy: 98.94%\n",
      "25\tValidation loss: 0.043509\tBest loss: 0.028580\tAccuracy: 99.02%\n",
      "26\tValidation loss: 0.036137\tBest loss: 0.028580\tAccuracy: 98.98%\n",
      "27\tValidation loss: 0.036727\tBest loss: 0.028580\tAccuracy: 99.02%\n",
      "28\tValidation loss: 0.038159\tBest loss: 0.028580\tAccuracy: 99.14%\n",
      "29\tValidation loss: 0.035168\tBest loss: 0.028580\tAccuracy: 99.30%\n",
      "30\tValidation loss: 0.040438\tBest loss: 0.028580\tAccuracy: 99.22%\n",
      "31\tValidation loss: 0.038718\tBest loss: 0.028580\tAccuracy: 99.18%\n",
      "32\tValidation loss: 0.040459\tBest loss: 0.028580\tAccuracy: 99.14%\n",
      "33\tValidation loss: 0.047244\tBest loss: 0.028580\tAccuracy: 98.98%\n",
      "34\tValidation loss: 0.037247\tBest loss: 0.028580\tAccuracy: 99.06%\n",
      "35\tValidation loss: 0.036001\tBest loss: 0.028580\tAccuracy: 99.30%\n",
      "36\tValidation loss: 0.034582\tBest loss: 0.028580\tAccuracy: 99.18%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125346ea0>,\n",
       "       batch_norm_momentum=0.95, batch_size=500, dropout_rate=None,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x122b289d8>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_bn = DNNClassifier(activation = leaky_relu(alpha = 0.1), batch_size = 500, learning_rate = 0.01, n_neurons = 90, random_state = 42, batch_norm_momentum = 0.95)\n",
    "dnn_clf_bn.fit(X_train1, y_train1, n_epochs = 1000, X_valid = X_valid1, y_valid = y_valid1)\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99338392683401444"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.060085\tBest loss: 0.060085\tAccuracy: 98.40%\n",
      "1\tValidation loss: 0.043903\tBest loss: 0.043903\tAccuracy: 98.48%\n",
      "2\tValidation loss: 0.032675\tBest loss: 0.032675\tAccuracy: 98.94%\n",
      "3\tValidation loss: 0.058024\tBest loss: 0.032675\tAccuracy: 98.36%\n",
      "4\tValidation loss: 0.039046\tBest loss: 0.032675\tAccuracy: 98.91%\n",
      "5\tValidation loss: 0.039813\tBest loss: 0.032675\tAccuracy: 98.91%\n",
      "6\tValidation loss: 0.036854\tBest loss: 0.032675\tAccuracy: 98.94%\n",
      "7\tValidation loss: 0.038086\tBest loss: 0.032675\tAccuracy: 99.30%\n",
      "8\tValidation loss: 0.034529\tBest loss: 0.032675\tAccuracy: 99.10%\n",
      "9\tValidation loss: 0.034710\tBest loss: 0.032675\tAccuracy: 99.02%\n",
      "10\tValidation loss: 0.049306\tBest loss: 0.032675\tAccuracy: 98.94%\n",
      "11\tValidation loss: 0.032885\tBest loss: 0.032675\tAccuracy: 99.22%\n",
      "12\tValidation loss: 0.037052\tBest loss: 0.032675\tAccuracy: 99.22%\n",
      "13\tValidation loss: 0.033327\tBest loss: 0.032675\tAccuracy: 99.14%\n",
      "14\tValidation loss: 0.032779\tBest loss: 0.032675\tAccuracy: 99.37%\n",
      "15\tValidation loss: 0.030975\tBest loss: 0.030975\tAccuracy: 99.26%\n",
      "16\tValidation loss: 0.031437\tBest loss: 0.030975\tAccuracy: 99.26%\n",
      "17\tValidation loss: 0.041874\tBest loss: 0.030975\tAccuracy: 99.14%\n",
      "18\tValidation loss: 0.023685\tBest loss: 0.023685\tAccuracy: 99.49%\n",
      "19\tValidation loss: 0.029426\tBest loss: 0.023685\tAccuracy: 99.37%\n",
      "20\tValidation loss: 0.034544\tBest loss: 0.023685\tAccuracy: 99.10%\n",
      "21\tValidation loss: 0.033540\tBest loss: 0.023685\tAccuracy: 99.37%\n",
      "22\tValidation loss: 0.031869\tBest loss: 0.023685\tAccuracy: 99.10%\n",
      "23\tValidation loss: 0.028856\tBest loss: 0.023685\tAccuracy: 99.34%\n",
      "24\tValidation loss: 0.042321\tBest loss: 0.023685\tAccuracy: 99.22%\n",
      "25\tValidation loss: 0.044440\tBest loss: 0.023685\tAccuracy: 99.18%\n",
      "26\tValidation loss: 0.039770\tBest loss: 0.023685\tAccuracy: 99.06%\n",
      "27\tValidation loss: 0.034723\tBest loss: 0.023685\tAccuracy: 99.22%\n",
      "28\tValidation loss: 0.032010\tBest loss: 0.023685\tAccuracy: 99.53%\n",
      "29\tValidation loss: 0.032153\tBest loss: 0.023685\tAccuracy: 99.06%\n",
      "30\tValidation loss: 0.031856\tBest loss: 0.023685\tAccuracy: 99.37%\n",
      "31\tValidation loss: 0.028770\tBest loss: 0.023685\tAccuracy: 99.41%\n",
      "32\tValidation loss: 0.026768\tBest loss: 0.023685\tAccuracy: 99.41%\n",
      "33\tValidation loss: 0.035105\tBest loss: 0.023685\tAccuracy: 99.06%\n",
      "34\tValidation loss: 0.026834\tBest loss: 0.023685\tAccuracy: 99.45%\n",
      "35\tValidation loss: 0.026453\tBest loss: 0.023685\tAccuracy: 99.37%\n",
      "36\tValidation loss: 0.025993\tBest loss: 0.023685\tAccuracy: 99.45%\n",
      "37\tValidation loss: 0.030846\tBest loss: 0.023685\tAccuracy: 99.34%\n",
      "38\tValidation loss: 0.037579\tBest loss: 0.023685\tAccuracy: 99.14%\n",
      "39\tValidation loss: 0.038023\tBest loss: 0.023685\tAccuracy: 99.10%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function relu at 0x11b8d3ae8>,\n",
       "       batch_norm_momentum=0.98, batch_size=100, dropout_rate=None,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x122b289d8>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=160,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_dn = DNNClassifier(activation=tf.nn.relu, batch_size=100, learning_rate=0.01,\n",
    "                        n_neurons=160, random_state=42, batch_norm_momentum = 0.98)\n",
    "dnn_clf_dn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99338392683401444"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99718239532063624"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf.predict(X_train1)\n",
    "accuracy_score(y_train1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.189585\tBest loss: 0.189585\tAccuracy: 95.50%\n",
      "1\tValidation loss: 0.128796\tBest loss: 0.128796\tAccuracy: 96.79%\n",
      "2\tValidation loss: 0.121250\tBest loss: 0.121250\tAccuracy: 96.95%\n",
      "3\tValidation loss: 0.103175\tBest loss: 0.103175\tAccuracy: 97.19%\n",
      "4\tValidation loss: 0.113290\tBest loss: 0.103175\tAccuracy: 97.46%\n",
      "5\tValidation loss: 0.086214\tBest loss: 0.086214\tAccuracy: 97.58%\n",
      "6\tValidation loss: 0.090685\tBest loss: 0.086214\tAccuracy: 97.81%\n",
      "7\tValidation loss: 0.087019\tBest loss: 0.086214\tAccuracy: 97.85%\n",
      "8\tValidation loss: 0.083561\tBest loss: 0.083561\tAccuracy: 97.54%\n",
      "9\tValidation loss: 0.086794\tBest loss: 0.083561\tAccuracy: 97.65%\n",
      "10\tValidation loss: 0.084195\tBest loss: 0.083561\tAccuracy: 97.77%\n",
      "11\tValidation loss: 0.077262\tBest loss: 0.077262\tAccuracy: 97.77%\n",
      "12\tValidation loss: 0.089691\tBest loss: 0.077262\tAccuracy: 97.93%\n",
      "13\tValidation loss: 0.070459\tBest loss: 0.070459\tAccuracy: 97.97%\n",
      "14\tValidation loss: 0.077419\tBest loss: 0.070459\tAccuracy: 97.97%\n",
      "15\tValidation loss: 0.071864\tBest loss: 0.070459\tAccuracy: 97.97%\n",
      "16\tValidation loss: 0.074455\tBest loss: 0.070459\tAccuracy: 98.20%\n",
      "17\tValidation loss: 0.074038\tBest loss: 0.070459\tAccuracy: 98.16%\n",
      "18\tValidation loss: 0.081956\tBest loss: 0.070459\tAccuracy: 97.97%\n",
      "19\tValidation loss: 0.079262\tBest loss: 0.070459\tAccuracy: 98.20%\n",
      "20\tValidation loss: 0.084982\tBest loss: 0.070459\tAccuracy: 97.50%\n",
      "21\tValidation loss: 0.076775\tBest loss: 0.070459\tAccuracy: 97.81%\n",
      "22\tValidation loss: 0.072419\tBest loss: 0.070459\tAccuracy: 98.20%\n",
      "23\tValidation loss: 0.074430\tBest loss: 0.070459\tAccuracy: 97.93%\n",
      "24\tValidation loss: 0.067777\tBest loss: 0.067777\tAccuracy: 98.20%\n",
      "25\tValidation loss: 0.069144\tBest loss: 0.067777\tAccuracy: 97.97%\n",
      "26\tValidation loss: 0.084158\tBest loss: 0.067777\tAccuracy: 98.01%\n",
      "27\tValidation loss: 0.071539\tBest loss: 0.067777\tAccuracy: 98.40%\n",
      "28\tValidation loss: 0.066924\tBest loss: 0.066924\tAccuracy: 98.36%\n",
      "29\tValidation loss: 0.137014\tBest loss: 0.066924\tAccuracy: 96.76%\n",
      "30\tValidation loss: 0.162304\tBest loss: 0.066924\tAccuracy: 96.44%\n",
      "31\tValidation loss: 0.203982\tBest loss: 0.066924\tAccuracy: 95.66%\n",
      "32\tValidation loss: 0.158688\tBest loss: 0.066924\tAccuracy: 95.74%\n",
      "33\tValidation loss: 0.204698\tBest loss: 0.066924\tAccuracy: 92.57%\n",
      "34\tValidation loss: 0.173460\tBest loss: 0.066924\tAccuracy: 96.01%\n",
      "35\tValidation loss: 0.157263\tBest loss: 0.066924\tAccuracy: 96.13%\n",
      "36\tValidation loss: 0.171539\tBest loss: 0.066924\tAccuracy: 94.18%\n",
      "37\tValidation loss: 0.135074\tBest loss: 0.066924\tAccuracy: 95.86%\n",
      "38\tValidation loss: 0.116741\tBest loss: 0.066924\tAccuracy: 97.15%\n",
      "39\tValidation loss: 0.122754\tBest loss: 0.066924\tAccuracy: 96.40%\n",
      "40\tValidation loss: 0.112843\tBest loss: 0.066924\tAccuracy: 96.52%\n",
      "41\tValidation loss: 0.104578\tBest loss: 0.066924\tAccuracy: 96.91%\n",
      "42\tValidation loss: 0.105849\tBest loss: 0.066924\tAccuracy: 97.46%\n",
      "43\tValidation loss: 0.104144\tBest loss: 0.066924\tAccuracy: 97.73%\n",
      "44\tValidation loss: 0.114987\tBest loss: 0.066924\tAccuracy: 97.65%\n",
      "45\tValidation loss: 0.098406\tBest loss: 0.066924\tAccuracy: 97.11%\n",
      "46\tValidation loss: 0.091322\tBest loss: 0.066924\tAccuracy: 97.85%\n",
      "47\tValidation loss: 0.089439\tBest loss: 0.066924\tAccuracy: 97.54%\n",
      "48\tValidation loss: 0.081308\tBest loss: 0.066924\tAccuracy: 97.93%\n",
      "49\tValidation loss: 0.111411\tBest loss: 0.066924\tAccuracy: 97.15%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125126ae8>,\n",
       "       batch_norm_momentum=None, batch_size=500, dropout_rate=0.5,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x122b289d8>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_dropout = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                                n_neurons=90, random_state=42,\n",
    "                                dropout_rate=0.5)\n",
    "dnn_clf_dropout.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98579490173185447"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.131360\tBest loss: 0.131360\tAccuracy: 95.86%\n",
      "1\tValidation loss: 0.125169\tBest loss: 0.125169\tAccuracy: 96.36%\n",
      "2\tValidation loss: 0.104122\tBest loss: 0.104122\tAccuracy: 97.46%\n",
      "3\tValidation loss: 0.095295\tBest loss: 0.095295\tAccuracy: 97.22%\n",
      "4\tValidation loss: 0.085382\tBest loss: 0.085382\tAccuracy: 97.69%\n",
      "5\tValidation loss: 0.075404\tBest loss: 0.075404\tAccuracy: 98.12%\n",
      "6\tValidation loss: 0.076496\tBest loss: 0.075404\tAccuracy: 97.89%\n",
      "7\tValidation loss: 0.081833\tBest loss: 0.075404\tAccuracy: 97.69%\n",
      "8\tValidation loss: 0.078271\tBest loss: 0.075404\tAccuracy: 97.89%\n",
      "9\tValidation loss: 0.082048\tBest loss: 0.075404\tAccuracy: 97.73%\n",
      "10\tValidation loss: 0.075823\tBest loss: 0.075404\tAccuracy: 98.12%\n",
      "11\tValidation loss: 0.075037\tBest loss: 0.075037\tAccuracy: 97.81%\n",
      "12\tValidation loss: 0.065717\tBest loss: 0.065717\tAccuracy: 98.24%\n",
      "13\tValidation loss: 0.065187\tBest loss: 0.065187\tAccuracy: 98.24%\n",
      "14\tValidation loss: 0.068554\tBest loss: 0.065187\tAccuracy: 98.16%\n",
      "15\tValidation loss: 0.063630\tBest loss: 0.063630\tAccuracy: 98.40%\n",
      "16\tValidation loss: 0.063939\tBest loss: 0.063630\tAccuracy: 98.24%\n",
      "17\tValidation loss: 0.065243\tBest loss: 0.063630\tAccuracy: 98.24%\n",
      "18\tValidation loss: 0.061100\tBest loss: 0.061100\tAccuracy: 98.44%\n",
      "19\tValidation loss: 0.061764\tBest loss: 0.061100\tAccuracy: 98.36%\n",
      "20\tValidation loss: 0.062193\tBest loss: 0.061100\tAccuracy: 98.24%\n",
      "21\tValidation loss: 0.057080\tBest loss: 0.057080\tAccuracy: 98.36%\n",
      "22\tValidation loss: 0.057979\tBest loss: 0.057080\tAccuracy: 98.40%\n",
      "23\tValidation loss: 0.059683\tBest loss: 0.057080\tAccuracy: 98.51%\n",
      "24\tValidation loss: 0.063865\tBest loss: 0.057080\tAccuracy: 98.28%\n",
      "25\tValidation loss: 0.061060\tBest loss: 0.057080\tAccuracy: 98.44%\n",
      "26\tValidation loss: 0.062517\tBest loss: 0.057080\tAccuracy: 98.51%\n",
      "27\tValidation loss: 0.057460\tBest loss: 0.057080\tAccuracy: 98.48%\n",
      "28\tValidation loss: 0.056335\tBest loss: 0.056335\tAccuracy: 98.75%\n",
      "29\tValidation loss: 0.060142\tBest loss: 0.056335\tAccuracy: 98.51%\n",
      "30\tValidation loss: 0.064310\tBest loss: 0.056335\tAccuracy: 98.36%\n",
      "31\tValidation loss: 0.063626\tBest loss: 0.056335\tAccuracy: 98.48%\n",
      "32\tValidation loss: 0.065130\tBest loss: 0.056335\tAccuracy: 98.32%\n",
      "33\tValidation loss: 0.057139\tBest loss: 0.056335\tAccuracy: 98.63%\n",
      "34\tValidation loss: 0.065815\tBest loss: 0.056335\tAccuracy: 98.20%\n",
      "35\tValidation loss: 0.061850\tBest loss: 0.056335\tAccuracy: 98.40%\n",
      "36\tValidation loss: 0.060106\tBest loss: 0.056335\tAccuracy: 98.44%\n",
      "37\tValidation loss: 0.056811\tBest loss: 0.056335\tAccuracy: 98.59%\n",
      "38\tValidation loss: 0.056912\tBest loss: 0.056335\tAccuracy: 98.63%\n",
      "39\tValidation loss: 0.059924\tBest loss: 0.056335\tAccuracy: 98.59%\n",
      "40\tValidation loss: 0.053372\tBest loss: 0.053372\tAccuracy: 98.59%\n",
      "41\tValidation loss: 0.055229\tBest loss: 0.053372\tAccuracy: 98.55%\n",
      "42\tValidation loss: 0.054237\tBest loss: 0.053372\tAccuracy: 98.59%\n",
      "43\tValidation loss: 0.057712\tBest loss: 0.053372\tAccuracy: 98.36%\n",
      "44\tValidation loss: 0.051432\tBest loss: 0.051432\tAccuracy: 98.71%\n",
      "45\tValidation loss: 0.057618\tBest loss: 0.051432\tAccuracy: 98.44%\n",
      "46\tValidation loss: 0.054632\tBest loss: 0.051432\tAccuracy: 98.63%\n",
      "47\tValidation loss: 0.053697\tBest loss: 0.051432\tAccuracy: 98.59%\n",
      "48\tValidation loss: 0.054198\tBest loss: 0.051432\tAccuracy: 98.71%\n",
      "49\tValidation loss: 0.057708\tBest loss: 0.051432\tAccuracy: 98.67%\n",
      "50\tValidation loss: 0.052593\tBest loss: 0.051432\tAccuracy: 98.67%\n",
      "51\tValidation loss: 0.054197\tBest loss: 0.051432\tAccuracy: 98.48%\n",
      "52\tValidation loss: 0.051821\tBest loss: 0.051432\tAccuracy: 98.51%\n",
      "53\tValidation loss: 0.058626\tBest loss: 0.051432\tAccuracy: 98.55%\n",
      "54\tValidation loss: 0.058388\tBest loss: 0.051432\tAccuracy: 98.44%\n",
      "55\tValidation loss: 0.052917\tBest loss: 0.051432\tAccuracy: 98.51%\n",
      "56\tValidation loss: 0.053128\tBest loss: 0.051432\tAccuracy: 98.55%\n",
      "57\tValidation loss: 0.050437\tBest loss: 0.050437\tAccuracy: 98.71%\n",
      "58\tValidation loss: 0.058561\tBest loss: 0.050437\tAccuracy: 98.51%\n",
      "59\tValidation loss: 0.053165\tBest loss: 0.050437\tAccuracy: 98.63%\n",
      "60\tValidation loss: 0.049435\tBest loss: 0.049435\tAccuracy: 98.71%\n",
      "61\tValidation loss: 0.053396\tBest loss: 0.049435\tAccuracy: 98.67%\n",
      "62\tValidation loss: 0.052337\tBest loss: 0.049435\tAccuracy: 98.55%\n",
      "63\tValidation loss: 0.053237\tBest loss: 0.049435\tAccuracy: 98.71%\n",
      "64\tValidation loss: 0.051932\tBest loss: 0.049435\tAccuracy: 98.67%\n",
      "65\tValidation loss: 0.051888\tBest loss: 0.049435\tAccuracy: 98.63%\n",
      "66\tValidation loss: 0.049818\tBest loss: 0.049435\tAccuracy: 98.59%\n",
      "67\tValidation loss: 0.054478\tBest loss: 0.049435\tAccuracy: 98.63%\n",
      "68\tValidation loss: 0.052086\tBest loss: 0.049435\tAccuracy: 98.48%\n",
      "69\tValidation loss: 0.061230\tBest loss: 0.049435\tAccuracy: 98.48%\n",
      "70\tValidation loss: 0.059926\tBest loss: 0.049435\tAccuracy: 98.44%\n",
      "71\tValidation loss: 0.058525\tBest loss: 0.049435\tAccuracy: 98.44%\n",
      "72\tValidation loss: 0.056201\tBest loss: 0.049435\tAccuracy: 98.48%\n",
      "73\tValidation loss: 0.056839\tBest loss: 0.049435\tAccuracy: 98.55%\n",
      "74\tValidation loss: 0.050602\tBest loss: 0.049435\tAccuracy: 98.55%\n",
      "75\tValidation loss: 0.057780\tBest loss: 0.049435\tAccuracy: 98.67%\n",
      "76\tValidation loss: 0.056801\tBest loss: 0.049435\tAccuracy: 98.55%\n",
      "77\tValidation loss: 0.057827\tBest loss: 0.049435\tAccuracy: 98.51%\n",
      "78\tValidation loss: 0.063401\tBest loss: 0.049435\tAccuracy: 98.51%\n",
      "79\tValidation loss: 0.055589\tBest loss: 0.049435\tAccuracy: 98.75%\n",
      "80\tValidation loss: 0.062477\tBest loss: 0.049435\tAccuracy: 98.59%\n",
      "81\tValidation loss: 0.055539\tBest loss: 0.049435\tAccuracy: 98.36%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x12372c158>,\n",
       "       batch_norm_momentum=None, batch_size=500, dropout_rate=0.4,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x122b289d8>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=50,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_dropout = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                                n_neurons=50, random_state=42,\n",
    "                                dropout_rate=0.4)\n",
    "dnn_clf_dropout.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98715703444249858"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"./my_best_mnist_model_0_to_4.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss:0\")\n",
    "Y_proba = tf.get_default_graph().get_tensor_by_name(\"Y_proba:0\")\n",
    "logits = Y_proba.op.inputs[0]\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"accuracy:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4\n",
      "0\tValidation loss: 0.989318\tBest loss: 0.989318\tAccuracy: 58.00%\n",
      "1\tValidation loss: 0.814950\tBest loss: 0.814950\tAccuracy: 65.33%\n",
      "2\tValidation loss: 0.851768\tBest loss: 0.814950\tAccuracy: 66.67%\n",
      "3\tValidation loss: 0.856994\tBest loss: 0.814950\tAccuracy: 65.33%\n",
      "4\tValidation loss: 0.751096\tBest loss: 0.751096\tAccuracy: 68.00%\n",
      "5\tValidation loss: 0.753933\tBest loss: 0.751096\tAccuracy: 68.67%\n",
      "6\tValidation loss: 0.815393\tBest loss: 0.751096\tAccuracy: 66.67%\n",
      "7\tValidation loss: 0.762574\tBest loss: 0.751096\tAccuracy: 70.00%\n",
      "8\tValidation loss: 0.823795\tBest loss: 0.751096\tAccuracy: 66.00%\n",
      "9\tValidation loss: 0.804963\tBest loss: 0.751096\tAccuracy: 68.67%\n",
      "10\tValidation loss: 0.796525\tBest loss: 0.751096\tAccuracy: 69.33%\n",
      "11\tValidation loss: 0.803264\tBest loss: 0.751096\tAccuracy: 67.33%\n",
      "12\tValidation loss: 0.869587\tBest loss: 0.751096\tAccuracy: 64.00%\n",
      "13\tValidation loss: 0.877949\tBest loss: 0.751096\tAccuracy: 68.00%\n",
      "14\tValidation loss: 0.799018\tBest loss: 0.751096\tAccuracy: 68.67%\n",
      "15\tValidation loss: 0.724509\tBest loss: 0.724509\tAccuracy: 74.00%\n",
      "16\tValidation loss: 0.685697\tBest loss: 0.685697\tAccuracy: 74.67%\n",
      "17\tValidation loss: 0.722147\tBest loss: 0.685697\tAccuracy: 72.67%\n",
      "18\tValidation loss: 0.710889\tBest loss: 0.685697\tAccuracy: 72.67%\n",
      "19\tValidation loss: 0.749971\tBest loss: 0.685697\tAccuracy: 68.67%\n",
      "20\tValidation loss: 0.724119\tBest loss: 0.685697\tAccuracy: 73.33%\n",
      "21\tValidation loss: 0.733974\tBest loss: 0.685697\tAccuracy: 69.33%\n",
      "22\tValidation loss: 0.719191\tBest loss: 0.685697\tAccuracy: 72.67%\n",
      "23\tValidation loss: 0.788883\tBest loss: 0.685697\tAccuracy: 68.67%\n",
      "24\tValidation loss: 0.743778\tBest loss: 0.685697\tAccuracy: 72.00%\n",
      "25\tValidation loss: 0.772395\tBest loss: 0.685697\tAccuracy: 72.67%\n",
      "26\tValidation loss: 0.695046\tBest loss: 0.685697\tAccuracy: 76.67%\n",
      "27\tValidation loss: 0.747637\tBest loss: 0.685697\tAccuracy: 72.00%\n",
      "28\tValidation loss: 0.749566\tBest loss: 0.685697\tAccuracy: 73.33%\n",
      "29\tValidation loss: 0.750000\tBest loss: 0.685697\tAccuracy: 71.33%\n",
      "30\tValidation loss: 0.713255\tBest loss: 0.685697\tAccuracy: 74.00%\n",
      "31\tValidation loss: 0.672673\tBest loss: 0.672673\tAccuracy: 76.00%\n",
      "32\tValidation loss: 0.734758\tBest loss: 0.672673\tAccuracy: 72.67%\n",
      "33\tValidation loss: 0.719945\tBest loss: 0.672673\tAccuracy: 74.00%\n",
      "34\tValidation loss: 0.753770\tBest loss: 0.672673\tAccuracy: 72.00%\n",
      "35\tValidation loss: 0.767107\tBest loss: 0.672673\tAccuracy: 72.67%\n",
      "36\tValidation loss: 0.733826\tBest loss: 0.672673\tAccuracy: 74.67%\n",
      "37\tValidation loss: 0.742230\tBest loss: 0.672673\tAccuracy: 73.33%\n",
      "38\tValidation loss: 0.689103\tBest loss: 0.672673\tAccuracy: 75.33%\n",
      "39\tValidation loss: 0.775630\tBest loss: 0.672673\tAccuracy: 71.33%\n",
      "40\tValidation loss: 0.705169\tBest loss: 0.672673\tAccuracy: 74.00%\n",
      "41\tValidation loss: 0.750791\tBest loss: 0.672673\tAccuracy: 74.67%\n",
      "42\tValidation loss: 0.678396\tBest loss: 0.672673\tAccuracy: 76.00%\n",
      "43\tValidation loss: 0.842389\tBest loss: 0.672673\tAccuracy: 72.00%\n",
      "44\tValidation loss: 0.662253\tBest loss: 0.662253\tAccuracy: 78.00%\n",
      "45\tValidation loss: 0.762558\tBest loss: 0.662253\tAccuracy: 71.33%\n",
      "46\tValidation loss: 0.763841\tBest loss: 0.662253\tAccuracy: 74.00%\n",
      "47\tValidation loss: 0.736624\tBest loss: 0.662253\tAccuracy: 75.33%\n",
      "48\tValidation loss: 0.668105\tBest loss: 0.662253\tAccuracy: 78.00%\n",
      "49\tValidation loss: 0.721788\tBest loss: 0.662253\tAccuracy: 76.00%\n",
      "50\tValidation loss: 0.802345\tBest loss: 0.662253\tAccuracy: 72.00%\n",
      "51\tValidation loss: 0.708236\tBest loss: 0.662253\tAccuracy: 76.00%\n",
      "52\tValidation loss: 0.797008\tBest loss: 0.662253\tAccuracy: 75.33%\n",
      "53\tValidation loss: 0.735417\tBest loss: 0.662253\tAccuracy: 74.00%\n",
      "54\tValidation loss: 0.700903\tBest loss: 0.662253\tAccuracy: 76.67%\n",
      "55\tValidation loss: 0.749797\tBest loss: 0.662253\tAccuracy: 74.67%\n",
      "56\tValidation loss: 0.779854\tBest loss: 0.662253\tAccuracy: 74.00%\n",
      "57\tValidation loss: 0.982159\tBest loss: 0.662253\tAccuracy: 70.67%\n",
      "58\tValidation loss: 0.814689\tBest loss: 0.662253\tAccuracy: 74.00%\n",
      "59\tValidation loss: 0.750207\tBest loss: 0.662253\tAccuracy: 71.33%\n",
      "60\tValidation loss: 0.751751\tBest loss: 0.662253\tAccuracy: 75.33%\n",
      "61\tValidation loss: 0.681022\tBest loss: 0.662253\tAccuracy: 76.67%\n",
      "62\tValidation loss: 0.710676\tBest loss: 0.662253\tAccuracy: 76.00%\n",
      "63\tValidation loss: 0.657750\tBest loss: 0.657750\tAccuracy: 78.67%\n",
      "64\tValidation loss: 0.778706\tBest loss: 0.657750\tAccuracy: 74.00%\n",
      "65\tValidation loss: 0.732945\tBest loss: 0.657750\tAccuracy: 77.33%\n",
      "66\tValidation loss: 0.749957\tBest loss: 0.657750\tAccuracy: 76.67%\n",
      "67\tValidation loss: 0.742787\tBest loss: 0.657750\tAccuracy: 76.00%\n",
      "68\tValidation loss: 0.735916\tBest loss: 0.657750\tAccuracy: 76.67%\n",
      "69\tValidation loss: 0.697453\tBest loss: 0.657750\tAccuracy: 78.00%\n",
      "70\tValidation loss: 0.787798\tBest loss: 0.657750\tAccuracy: 72.00%\n",
      "71\tValidation loss: 0.729946\tBest loss: 0.657750\tAccuracy: 77.33%\n",
      "72\tValidation loss: 0.710900\tBest loss: 0.657750\tAccuracy: 76.67%\n",
      "73\tValidation loss: 0.830788\tBest loss: 0.657750\tAccuracy: 73.33%\n",
      "74\tValidation loss: 0.752674\tBest loss: 0.657750\tAccuracy: 76.00%\n",
      "75\tValidation loss: 0.787903\tBest loss: 0.657750\tAccuracy: 73.33%\n",
      "76\tValidation loss: 0.792982\tBest loss: 0.657750\tAccuracy: 72.00%\n",
      "77\tValidation loss: 0.768189\tBest loss: 0.657750\tAccuracy: 74.67%\n",
      "78\tValidation loss: 0.717930\tBest loss: 0.657750\tAccuracy: 78.00%\n",
      "79\tValidation loss: 0.854800\tBest loss: 0.657750\tAccuracy: 72.67%\n",
      "80\tValidation loss: 0.715780\tBest loss: 0.657750\tAccuracy: 76.00%\n",
      "81\tValidation loss: 0.800379\tBest loss: 0.657750\tAccuracy: 74.00%\n",
      "82\tValidation loss: 0.902380\tBest loss: 0.657750\tAccuracy: 70.00%\n",
      "83\tValidation loss: 0.730698\tBest loss: 0.657750\tAccuracy: 75.33%\n",
      "Early stopping!\n",
      "Total training time: 11.7s\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 77.86%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_best_mnist_model_0_to_4\")\n",
    "    for var in output_layer_vars:\n",
    "        var.initializer.run()\n",
    "\n",
    "    t0 = time.time()\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden5_out = tf.get_default_graph().get_tensor_by_name('hidden5_out:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4\n",
      "0\tValidation loss: 0.878946\tBest loss: 0.878946\tAccuracy: 65.33%\n",
      "1\tValidation loss: 0.869923\tBest loss: 0.869923\tAccuracy: 65.33%\n",
      "2\tValidation loss: 0.828193\tBest loss: 0.828193\tAccuracy: 65.33%\n",
      "3\tValidation loss: 0.802410\tBest loss: 0.802410\tAccuracy: 68.00%\n",
      "4\tValidation loss: 0.822666\tBest loss: 0.802410\tAccuracy: 67.33%\n",
      "5\tValidation loss: 0.828255\tBest loss: 0.802410\tAccuracy: 65.33%\n",
      "6\tValidation loss: 1.032599\tBest loss: 0.802410\tAccuracy: 60.67%\n",
      "7\tValidation loss: 0.784273\tBest loss: 0.784273\tAccuracy: 68.67%\n",
      "8\tValidation loss: 0.764373\tBest loss: 0.764373\tAccuracy: 68.67%\n",
      "9\tValidation loss: 0.711155\tBest loss: 0.711155\tAccuracy: 70.67%\n",
      "10\tValidation loss: 0.729085\tBest loss: 0.711155\tAccuracy: 72.00%\n",
      "11\tValidation loss: 0.797304\tBest loss: 0.711155\tAccuracy: 68.00%\n",
      "12\tValidation loss: 0.859240\tBest loss: 0.711155\tAccuracy: 63.33%\n",
      "13\tValidation loss: 0.750113\tBest loss: 0.711155\tAccuracy: 72.00%\n",
      "14\tValidation loss: 0.753698\tBest loss: 0.711155\tAccuracy: 72.00%\n",
      "15\tValidation loss: 0.765634\tBest loss: 0.711155\tAccuracy: 70.67%\n",
      "16\tValidation loss: 0.735671\tBest loss: 0.711155\tAccuracy: 70.67%\n",
      "17\tValidation loss: 0.689563\tBest loss: 0.689563\tAccuracy: 76.00%\n",
      "18\tValidation loss: 0.697272\tBest loss: 0.689563\tAccuracy: 75.33%\n",
      "19\tValidation loss: 0.671747\tBest loss: 0.671747\tAccuracy: 74.67%\n",
      "20\tValidation loss: 0.767654\tBest loss: 0.671747\tAccuracy: 68.00%\n",
      "21\tValidation loss: 0.764816\tBest loss: 0.671747\tAccuracy: 69.33%\n",
      "22\tValidation loss: 0.765203\tBest loss: 0.671747\tAccuracy: 72.00%\n",
      "23\tValidation loss: 0.751196\tBest loss: 0.671747\tAccuracy: 73.33%\n",
      "24\tValidation loss: 0.713928\tBest loss: 0.671747\tAccuracy: 74.00%\n",
      "25\tValidation loss: 0.769807\tBest loss: 0.671747\tAccuracy: 70.67%\n",
      "26\tValidation loss: 0.748877\tBest loss: 0.671747\tAccuracy: 72.67%\n",
      "27\tValidation loss: 0.711313\tBest loss: 0.671747\tAccuracy: 74.67%\n",
      "28\tValidation loss: 0.699854\tBest loss: 0.671747\tAccuracy: 75.33%\n",
      "29\tValidation loss: 0.734055\tBest loss: 0.671747\tAccuracy: 74.00%\n",
      "30\tValidation loss: 0.775667\tBest loss: 0.671747\tAccuracy: 70.67%\n",
      "31\tValidation loss: 0.724548\tBest loss: 0.671747\tAccuracy: 75.33%\n",
      "32\tValidation loss: 0.674073\tBest loss: 0.671747\tAccuracy: 76.00%\n",
      "33\tValidation loss: 0.657724\tBest loss: 0.657724\tAccuracy: 76.00%\n",
      "34\tValidation loss: 0.715066\tBest loss: 0.657724\tAccuracy: 75.33%\n",
      "35\tValidation loss: 0.729425\tBest loss: 0.657724\tAccuracy: 76.00%\n",
      "36\tValidation loss: 0.742427\tBest loss: 0.657724\tAccuracy: 74.00%\n",
      "37\tValidation loss: 0.683101\tBest loss: 0.657724\tAccuracy: 75.33%\n",
      "38\tValidation loss: 0.688755\tBest loss: 0.657724\tAccuracy: 76.00%\n",
      "39\tValidation loss: 0.698772\tBest loss: 0.657724\tAccuracy: 76.00%\n",
      "40\tValidation loss: 0.724304\tBest loss: 0.657724\tAccuracy: 74.67%\n",
      "41\tValidation loss: 0.747857\tBest loss: 0.657724\tAccuracy: 73.33%\n",
      "42\tValidation loss: 0.790991\tBest loss: 0.657724\tAccuracy: 70.00%\n",
      "43\tValidation loss: 0.668126\tBest loss: 0.657724\tAccuracy: 78.00%\n",
      "44\tValidation loss: 0.724678\tBest loss: 0.657724\tAccuracy: 76.00%\n",
      "45\tValidation loss: 0.719958\tBest loss: 0.657724\tAccuracy: 74.67%\n",
      "46\tValidation loss: 0.705859\tBest loss: 0.657724\tAccuracy: 75.33%\n",
      "47\tValidation loss: 0.737972\tBest loss: 0.657724\tAccuracy: 74.67%\n",
      "48\tValidation loss: 0.815417\tBest loss: 0.657724\tAccuracy: 73.33%\n",
      "49\tValidation loss: 0.767702\tBest loss: 0.657724\tAccuracy: 73.33%\n",
      "50\tValidation loss: 0.687809\tBest loss: 0.657724\tAccuracy: 76.00%\n",
      "51\tValidation loss: 0.687014\tBest loss: 0.657724\tAccuracy: 77.33%\n",
      "52\tValidation loss: 0.743593\tBest loss: 0.657724\tAccuracy: 77.33%\n",
      "53\tValidation loss: 0.886016\tBest loss: 0.657724\tAccuracy: 70.67%\n",
      "Early stopping!\n",
      "Total training time: 13.6s\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 77.99%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_best_mnist_model_0_to_4\")\n",
    "    for var in output_layer_vars:\n",
    "        var.initializer.run()\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    hidden5_train = hidden5_out.eval(feed_dict={X: X_train2, y: y_train2})\n",
    "    hidden5_valid = hidden5_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            h5_batch, y_batch = hidden5_train[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={hidden5_out: h5_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={hidden5_out: hidden5_valid, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
